% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ,man,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{latent class analysis, mixture models, best practices, free open source software, tidySEM\newline\indent Word count: 8206}
\usepackage{csquotes}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Best Practices in Latent Class Analysis using Free Open Source Software},
  pdfauthor={Caspar J. van Lissa1,2},
  pdflang={en-EN},
  pdfkeywords={latent class analysis, mixture models, best practices, free open source software, tidySEM},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Best Practices in Latent Class Analysis using Free Open Source Software}
\author{Caspar J. van Lissa\textsuperscript{1,2}}
\date{}


\shorttitle{BEST PRACTICES LCA}

\authornote{

Correspondence concerning this article should be addressed to Caspar J. van Lissa, Padualaan 14, 3584CH Utrecht, The Netherlands. E-mail: \href{mailto:c.j.vanlissa@gmail.com}{\nolinkurl{c.j.vanlissa@gmail.com}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Utrecht University, Methodology \& Statistics\\\textsuperscript{2} Open Science Community Utrecht}

\abstract{%
Latent class analysis is a family of techniques for identifying groups in data based
on a parametric model. Its examples include mixture models, latent profile
analysis, latent class analysis with ordinal indicators, and latent class
growth analysis. Despite the popularity of this technique, there is limited
guidance with respect to best practices in estimating and reporting mixture
models. Moreover, although user-friendly interfaces for advanced mixture
modeling have long been available in commercial software packages, open
source alternatives have remained somewhat inaccessible. This tutorial
describes best practices for the estimation and reporting of latent class analysis,
using free and open source software in R. To this end, this tutorial
introduces new functionality for estimating and reporting mixture
models in the \texttt{tidySEM} R-package whose backend relies on
the \texttt{OpenMx} R-package.
}



\begin{document}
\maketitle

Latent class analysis (LCA) is an umbrella term
that refers to a number of techniques for estimating unobserved group membership
based on a parametric model of one or more observed indicators of group membership.
The types of LCA have become quite popular across scientific fields,
most notably finite Gaussian mixture modeling and latent profile analysis.
Vermunt, J.K. et al. (2004) defined LCA more generally as virtually any statistical model
where ``some of the parameters {[}\ldots{]} differ across unobserved subgroups''.

Despite its popularity, there is a lack of standards for estimating and reporting LCA.
While Van De Schoot, Sijbrandij, Winter, Depaoli, and Vermunt (2017) developed reporting guidelines
for a specific type of LCA known as latent growth models,
general reporting guidelines for all LCA are still lacking.
This complicates manuscript review and assessment of the quality of published studies,
and introduces a risk of misapplications of the technique.
The present paper seeks to address this gap in the literature
by suggesting updated guidelines for estimation and reporting of LCA,
based on current best practices.
Importantly, in order to lower the barrier of entry and ensure reproducibility of all examples,
this paper exclusively relies on free, open source software in R.
Our goal is to make best-practices in LCA widely accessible.

\hypertarget{defining-latent-class-analysis}{%
\subsection{Defining Latent Class Analysis}\label{defining-latent-class-analysis}}

Latent class analysis is a group of methods for estimating unobserved groups
based on a parametric model of observed indicators of group membership.
The concept of LCA can be understood in different ways.
A mixture model assumes that the study population
is composed of \(K\) subpopulations or classes.
It further assumes that the observed data are a mixture of data generated by class-specific models.
The simplest univariate model is a normal distribution,
which can be described with two parameters: the mean and the variance.
Commonly, the same model is estimated across all classes,
but with different parameters for each class (i.e., class-specific means and variances).
Mixture modeling then estimates both the parameters for each class,
and the probability that an individual belongs to each class.

As an illustrative example, imagine that a detective wants to know
if it would be possible to use mixture modeling to identify the sex of a suspect,
based on footprints found at the crime scene.
To test the feasibility of this approach,
the detective records the shoe sizes and sex of 100 volunteers.
The resulting observed data look like this:

\begin{figure}
\includegraphics[width=7in]{shoedens} \caption{Kernel density plot of shoe sizes.}\label{fig:shoedens}
\end{figure}

The distribution is evidently bimodal, which bodes well for the intended mixture model.
In this case, the number of classes is known a-priori.
When estimating a two-class mixture model, the detective observes that the model estimates
the mean shoe size of the two groups are equal to 7.25
and 9.22,
which is close to the true means of the two groups,
namely 9.04 and 6.93.
When tabulating estimated group membership against observed (known) group membership,
it can be seen that women are classified with a high degree of accuracy, but men are not:

\begin{table}

\caption{\label{tab:tabshoe}Observed group membership by estimated class membership.}
\centering
\begin{tabular}[t]{l|r|r}
\hline
Observed & Class 1 & Class 2\\
\hline
Man & 21 & 28\\
\hline
Woman & 51 & 0\\
\hline
\end{tabular}
\end{table}

A LCA is like confirmatory factor analysis,
except that the continuous latent variable is substituted with a categorical latent variable.
One difference between the two techniques is that
factor analysis can be considered as a way to group observed \emph{variables} into latent constructs,
and LCA groups \emph{individuals} into classes.
In line with this distinction, LCA is sometimes referred to as a ``person-centered'' technique,
and factor analysis as a ``variable-centered'' technique (Nylund-Gibson \& Choi, 2018).

When the focus is on the model parameters in each group,
LCA can be thought of as similar to a multi-group structural equation model.
The main distinction is that group membership is not known a-priori,
but is instead estimated -- with measurement error -- based on the data.
Whereas in a multi-group model, the data are split by group and treated as independent samples,
in LCA, all cases contribute to the estimation of all parameters in all groups.
The relative contribution of each case to the parameters of each group
is determined by that case's posterior probability of belonging to that group.

When the focus is on each individual's estimated class membership,
LCA can be thought of as a type of clustering algorithm.
In line with this perspective, LCA is sometimes described as ``model-based
clustering'' (Hennig, Meila, Murtagh, \& Rocci, 2015; Scrucca, Fop, Murphy, \& Raftery, 2016).
Many clustering algorithms apply some recursive splitting algorithm to the data.
By contrast ``model-based'' clustering refers to the fact that LCA
estimates cluster membership based on a parametric model.
Specifically, the posterior class probability that an individual belongs to a latent class
can be computed from the likelihood of that individual's observed data under given the class-specific model.

Finally, in the context of machine learning, LCA can be considered as an
\emph{unsupervised classification} problem (Figueiredo \& Jain, 2002).
The term \emph{unsupervised} refers to the fact that the outcome variable
-- true class membership -- is not known,
and the term \emph{classification} refers to the fact that
the algorithm is predicting a categorical outcome -- class membership.

\hypertarget{a-taxonomy-of-latent-class-analysis-methods}{%
\subsection{A Taxonomy of Latent Class Analysis Methods}\label{a-taxonomy-of-latent-class-analysis-methods}}

In this paper, we use the term latent class analysis to refer to
a family of techniques that estimate latent class membership
based on a parametric model of observed indicators.
From a historical perspective, the term was initially conceived to
refer to analyses with categorical, usually binary indicators (Vermunt, J.K. et al., 2004).
Nowadays, there are a number of related techniques,
known by distinct names, that serve a similar purpose.
The term ``latent class analysis'' seems most appropriate
as an umbrella term for this broader class of models,
as it only refers to the purpose of the analysis,
and does not imply restrictions to the model used,
or the level of measurement of the indicators.
Given the abundance of terms in use for closely related classes of models,
we will provide a rudimentary taxonomy of LCA methods.

A common type of LCA is the \emph{finite Gaussian mixture model};
a univariate analysis where the observed distribution of a single variable
is assumed to result from a mixture of a known number of normal (Gaussian) distributions.
The parameters of a finite Gaussian mixture model are
the means and variances of these underlying normal distributions.
The analysis of shoe sizes presented earlier is a canonical example of this type of analysis.
In the multivariate case, with more than one indicator variable,
the parameters of a mixture model are the means, variances, and covariances
between the indicators (which can be standardized to obtain correlations).
These parameters can be estimated freely, or set to be constrained across classes.

The technique known as \emph{latent profile analysis (LPA)} is a special case of the mixture model,
which assumes conditional independence of the indicators.
Conditional independence means that, after class membership is accounted for,
the covariances/correlations between indicators are assumed to be zero.
This can be conceived of as a restricted mixture model with covariances fixed to zero.
In some cases, such constraints will be inappropriate;
for instance when the cohesion between indicators is expected to differ between classes.
As an example, a LPA of ocean plastic particles
found two classes of particles based on length and width:
a class of smaller particles with a high correlation between length and width,
meaning that these particles were approximately round or square in shape,
and a class of larger particles with a low correlation between length and width,
meaning that these particles were heterogenous in shape.
From a theoretical perspective, this makes sense,
because the smaller particles have been ground down to a more uniform shape by the elements.

It is also possible to estimate a mixture model based on latent indicators.
This means that, within each class,
one or more continuous latent variables are estimated based on the observed indicators.
Categorical latent variable membership is then estimated
based on these continuous latent variables.
A common application of this approach is in longitudinal research,
where the indicators reflect one construct assessed at different time points.
Examples of this approach include \emph{growth mixture models} (GMM)
and \emph{latent class growth analyses} (LCGA).
These techniques estimate a latent growth model
to describe individual trajectories over time.
The growth mixture model is a latent class model
where the parameters that indicate class membership
are the intercepts and variances,
and typically covariances of the latent growth variables,
e.g., a latent intercept and slope.
This technique assumes that individuals within a class can have heterogenous trajectories.
If the variance of the growth parameters is fixed to zero,
it is known as a latent class growth analysis.
This latter approach assumes that all individuals within a class
share the same identical trajectory,
and that any variance in the indicators not explained by the class-specific latent trajectories
is due to residual error variance.

The term latent class analysis originally referred to cases
where the observed indicators were categorical.
Nowadays, it is more commonly used as an umbrella term.
To prevent ambiguity, the special case
where indicators are of binary or ordinal measurement level
might be described as \emph{latent class analysis with ordinal indicators}.
Latent class models with ordinal indicators are parameterized differently from mixture models.
One common parameterization assumes that
each categorical variable reflects an underlying standard normal distribution.
The parameters are ``thresholds'' that correspond to quantiles of a standard normal distribution
(with \(N(\mu = 0, \sigma = 1)\)).
These thresholds are estimated based on
the proportion of individuals in each of the response categories of the indicator variable.
For example, a binary indicator has a single threshold that distinguishes the two response categories.
If responses are distributed 50/50, then the corresponding threshold would be \(t_1 = 0.00\).
If the responses are distributed 60/40, then the resulting threshold would be \(t_1 = 0.25\).

This paper will primarily focus on mixture models and special cases thereof,
although most of the suggested guidelines are applicable to all LCA methods.

\hypertarget{use-cases-for-latent-class-analysis}{%
\subsection{Use Cases for Latent Class Analysis}\label{use-cases-for-latent-class-analysis}}

There are several use cases for which LCA methods are suitable.
One example is to test a theory that postulates the existence of a categorical latent variable.
For example, \emph{identity status theory} posits that, at any given point in time,
adolescents reside in one of four identity statuses.
Latent class analysis can be used to identify these four statuses
based on observed indicators (e.g., self-reported identity exploration and commitment).
If results indicate that the data are better described by a different number of classes,
or that the four-class solution does not correspond to the predicted pattern of responses on the indicators,
then the theory may be called into question.

Another use case is unsupervised learning:
when the goal is to restore unobserved class membership based on observed indicators,
or to classify individuals.
For example, a mixture model can be used as a diagnostic aid
when several clinical indicators can be used to distinguish between
a fixed number of physical (Baughman, Bisgard, Lynn, \& Meade, 2006) or mental (Wu, Woody, Yang, Pan, \& Blazer, 2011) health problems.
The example of shoe size is a rudimentary illustration of this type of application.

LCA can be used as a descriptive analysis
where a researcher wishes to describe their sample
and identify a few prototypes based on many variables.
As an example, if a survey among all Dutch academics was carried out
with the goal of bringing about a funding reform,
LCA could be used to discover the types of publications that get funding.

With LCA, our goal could be to inductively identify the number of classes.
For instance, if we believe that a variable represents a group,
but don't know how many groups there are,
LCA may be an appropriate technique to answer this question.
For example, Hopfer, Tan, and Wylie (2014) studied substance use, sexual behavior,
and mental health status of urban population in Winnipeg, Canada.
The underlying assumption was that there were different risk profiles,
but their number was not known.
From a collection of indicators, LCA provided evidence
that there are four distinct risk profiles in the Winnipeg area.

Another application of LCA is to classify individuals.
In a peer harassment study,
Giang and Graham (2008) used LCA to classify over 2,000 sixth grade students
into aggressor and victim latent classes.
The five-class solution comprised of classes of
victims, aggressors, and socially adjusted students.
For instance, it revealed that there were two types of victims:
highly-victimized aggressive-victims and highly-aggressive aggressive-victims.

LCA is also appropriate when we wish to identify indicators that capture classes well.
High quality indicators will be strongly related to the latent variable
and will lead to good class separation.
This relationship of high quality indicators to the latent variable
is reflected in very high or very low conditional response probabilities.
For a simulation study exploring the effects of indicator quality on LCA, see Geiser and Wurpts (2014).
Therefore, one could use conditional response probabilities for each item to assess its quality
with regards to how well it helps separate the latent classes.
From this, a theory about the selection of indicators could be informed.

An extension of LCA is that containing covariates which can be used to predict class membership.
In this approach, we not only model the latent class variable based on indicators,
but we also relate the class membership to other explanatory variables (Vermunt (2017)).
An example of using covariates comes from Nozadi et al. (2016)
who applied LCA to identify the probability of children's membership to an anxiety class.
The authors tested several covariates including children's age, sex, and accuracy scores.
Age and sex were not found to be related to the children's latent class membership,
hence these covariates were excluded from the analysis.
Accuracy scores, however, were related to probabilities of
being in anxiety and attention-anxiety classes.
Therefore this covariate was kept as a valuable predictor.

When our interest is the prediction of one or more outcomes,
LCA can be used to construct latent classes as categorical predictors.
Lanza, Tan, and Bray (2013) demonstrated how LCA can be used
to classify adolescents into depression classes,
and subsequently these classes can be used to predict smoking, grades, and delinquency.
The study showed that the outcomes predicted by class membership
can be binary (regular smoking), continuous (grades) or count (delinquency).

In addition to these applications, LCA can be used for dimensionality reduction
as the resulting groups summarize response patterns on a large number of indicators.
For example, MacGregor et al. (2021) investigated symptom profiles
among injured U.S. military personnel.
They used fifteen dichotomous items
from the Post-Deployment Health Assessment survey as LCA indicators.
Combinatorics informs us that fifteen dichotomous items have
\(2^{15}\) or \(32,768\) unique symptom combinations.
Perhaps for this reason, MacGregor et al. (2021) incorporated LCA
as a method of dimensionality reduction.
A five class solution was found to have the best fit
according to both statistical criteria and clinical interpretability.

Finally, LCA can be used to deal with data which violate certain assumptions.
As discussed in the shoe size example, LCA can deal with violations of normality.
In fact, LCA assumes the population distribution is a non-normal mixture
of \(K\) normal distributions,
and it can discover the value of \(K\), i.e.~generate a \(K\)-class solution.

\hypertarget{best-practices}{%
\section{Best Practices}\label{best-practices}}

\hypertarget{best-practices-in-estimation}{%
\subsection{Best Practices in Estimation}\label{best-practices-in-estimation}}

The best practices in estimation, as outlined in Table \ref{tab:checkest},
are rooted in existing recommendations for best practices
for estimating specific subtypes of LCA,
including latent class growth analysis (Van De Schoot et al., 2017)
and latent class analysis with ordinal indicators (e.g., Nylund-Gibson \& Choi, 2018).
These were generalized to be relevant to all types of LCA,
and updated to current best practices, as explained below.

\begin{tabular}[t]{l|l}
\hline
\# & Item\\
\hline
1. & Examining Observed Data\\
\hline
2. & Handling Missing Data\\
\hline
3. & Alternative Model Specifications\\
\hline
4. & Software\\
\hline
5. & Algorithm\\
\hline
6. & Class Enumeration\\
\hline
7. & Model Fit Indices\\
\hline
8. & Classification Diagnostics\\
\hline
9. & Interpreting Class Solutions\\
\hline
10. & Label switching\\
\hline
\end{tabular}

\hypertarget{examining-observed-data}{%
\subsubsection{Examining Observed Data}\label{examining-observed-data}}

Examining observed data is essential for any analysis
as it may reveal patterns and violations of assumptions
that had not been considered prior to data collection.
Special attention should be paid
to level of measurement of the indicators.
Finite Gaussian mixture models (including LPA)
are only suitable for continuous variables.
Indicators with an ordinal level of measurement
are likely to violate the assumption of
within-class normal distributions of mixture models
(see Vermunt, 2011).
Personal experience consulting on LCA methods and
moderating the \texttt{tidyLPA} Google group
suggest that the misapplication of mixture models
to ordinal (e.g., Likert-type) indicators
is the most common source of user error.
Whereas it has been argued that some parametric methods are robust
when scales with 7+ indicators are treated as continuous
(e.g., Norman, 2010),
this certainly does not imply that all methods are.
It is certainly unlikely that such ordinal variables
can be treated as a \emph{mixture} of multiple normal distributions.
The problem becomes egregious when
the number of classes estimated equals or exceeds the number of categories;
in this case, each class-specific mean could describe a single response category,
and a class-specific variance component would be nonsensical.
In sum, Likert-type scales are rarely suitable for mixture modeling;
latent class analysis with ordinal indicators is more appropriate.

Relatedly, a recent publication claimed that
an assumption of mixture models is that
observed indicators are normally distributed (Spurk, Hirschi, Wang, Valero, \& Kauffeld, 2020).
This is incorrect.
When the number of classes is greater than one,
mixture models assume that the observed indicators
are a mixture of multiple (multivariate) normal distributions.
In our shoe size example, it can be seen that the population distribution
is comprised of two normal distributions.
When examined visually, the population distribution is evidently bimodal.
The Shapiro-Wilk normality test (\(W = 0.971\), \(p < 0.05\) )
rejects the null hypothesis that
the sample comes from a normally distributed population.
Yet, this is a prototypical example of a mixed population distribution
where LCA can discover latent groups.
If the population distribution were instead normal,
there would be no classes to extract
as the whole population would belong to a single class.

Extensive descriptive statistics
(including the number of unique values,
variance of categorical variables, and missingness; see next subsection)
can be obtained using the function \texttt{tidySEM::descriptives(data)}.
Note, however, that sample-level descriptive statistics
are of limited value when the goal of a study is
to identify subsamples using latent class analysis.
Plots (density plots for continuous variables,
and bar charts for categorical ones) may be more diagnostic.
Note that density plots can also aid in the choice of the number of classes,
as further explained in the section on visualization.
Descriptive statistics and plots can be relegated to online supplements,
provided that these are readily accessible
(consider using a GitHub repository as a comprehensive public research archive,
as explained in Van Lissa et al., 2021).

\hypertarget{handling-missing-data}{%
\subsubsection{Handling Missing Data}\label{handling-missing-data}}

Previous work has emphasized the importance of examining
the pattern of missing data and
reporting how missingness was handled
(Van De Schoot et al., 2017).
Three types of missingness have been distinguished
in the literature (Rubin, 1976):
Missing completely at random (MCAR),
which means that missingness is random;
missing at random (MAR),
which means that missingness is contingent on the \emph{observed} data
(and can thus be accounted for);
and finally missing not at random (MNAR),
which means that missingness is related to unobserved factors.
It is possible to conduct a so-called ``MCAR'' test,
for example the non-parametric MCAR test (Jamshidian \& Jalal, 2010).
But note that the name ``MCAR test'' is somewhat misleading,
as the null-hypothesis of this test is that the data are not MAR,
and a significant test statistic indicates that
missingness is related to the observed data (MAR).
A non-significant test statistic does not distinguish between MCAR or MNAR.
As Little's classic MCAR test relies on
the comparison of variances across groups
with different patterns of missing data,
it assumes normality (Little, 1988).
This assumption is tenuous in the context of LCA.
A non-parametric MCAR test,
as provided by Jamshidian and Jalal,
may be more suitable (Jamshidian \& Jalal, 2010).
Unfortunately, this test was removed from the central R-repository CRAN
due to lack of maintenance.
For this tutorial, I have re-implemented it in the \texttt{mice} package
as \texttt{mice::mcar()},
with a fast backend in C++ and new printing and plotting methods.

While we concur that investigating missingness is due diligence,
it is important to emphasize that
missingness is adequately handled by default in many software packages for LCA,
such as Mplus, and \texttt{OpenMx} which is the backend of \texttt{tidySEM}.
These packages use Full Information Maximum Likelihood (FIML) estimation,
which makes use of all available information without imputing missing values.
FIML is a best-practice solution for handling missing data;
on par with multiple imputation (Lee \& Shi, 2021).
FIML estimation assumes that missingness is either MCAR or MAR.
Thus, one would typically proceed with FIML,
regardless of the outcome of an MCAR test.
Although FIML does not, by default, handle missingness in exogenous variables
-- all indicator variables in LCA are endogenous,
so this is not a concern.

Multiple imputation is less suitable to LCA for two reasons.
First, because LCA methods are often computationally expensive,
and conducting them on multiple imputed datasets may be unfeasible.
Second, because there is no straightforward way
to integrate LCA results across multiple datasets.
To conclude; our recommendation is to inspect missingness
(e.g., using \texttt{mice::MCAR()})
and report the proportion of missingness per variable
(e.g., using \texttt{tidySEM::descriptives()}),
before proceeding with FIML.
One minor concern is that the K-means algorithm,
which \texttt{tidySEM} uses for determining starting values,
is \emph{not} robust to missing values.
When it fails, \texttt{tidySEM} automatically switches to hierarchical clustering,
unless the user specifies a different
clustering algorithm or uses manual starting values.

\hypertarget{alternative-model-specifications}{%
\subsubsection{Alternative Model Specifications}\label{alternative-model-specifications}}

In order to aid researchers working with latent trajectory models,
Van De Schoot et al. (2017) developed a protocol called
Guidelines for Reporting on Latent Trajectory Studies (GRoLTS).
Two of the GRoLTS guidelines (namely, 6a and 6b) refer to
considering alternative model specifications.
Both are discussing specific cases in latent trajectory model specification.
The first is about whether the variance of the growth parameter
is estimated freely or fixed,
and the second is about whether conditional independence is assumed,
as the researcher might also want to free-up the variance-covariance structure.
In LCA, there are also many different ways to specify the model.
Means, variances, and covariances between the indicators
can either be constrained across classes, or estimated freely.
Researchers using LCA should transparently report their chosen parametrization
as well as discuss different parametrizations that were tested in the process.

Different types of latent class models have different parameters.
For example, mixture models and latent profile analyses
typically have class-specific means, variances, and covariances.
Latent growth analyses have the same parameters,
but with respect to the latent growth variables.
Latent class analyses with ordinal indicators have thresholds.
All of these parameters can be freely estimated,
constrained to be equal across classes,
or fixed to a certain value (e.g., to zero).
The total number of parameters thus
scales with the number of estimated classes.
Consequently, LCA methods have a potentially very high number of parameters.
As any of these parameters could be misspecified,
it is important to consider alternative model specifications.
However, alternative model specifications may be approached differently
depending on whether an analysis is data driven (exploratory),
or theory driven (confirmatory).
This distinction has remained underemphasized in prior writing.

Prior literature on LCA has emphasized its exploratory applications
(Nylund, Asparouhov, \& MuthÃ©n, 2007).
In exploratory LCA,
a large number of models are typically estimated in batch,
with varying numbers of classes and model specifications.
The ``correct'' model specification is then determined
based on a combination of fit indices,
significance tests, and interpretability.
For latent profile analysis,
the function \texttt{tidySEM::mx\_profiles(classes,\ variances,\ covariances)}
largely automates this process.
The argument \texttt{classes} indicates which
class solutions should be estimated (e.g., 1 through 6).
The argument \texttt{variances} specifies whether
variances should be \texttt{"equal"} or \texttt{"varying"} across classes.
The argument \texttt{covariances} specifies whether
covariances should be constrained to
\texttt{"zero"}, \texttt{"equal"} or \texttt{"varying"} across classes.
The means are free to vary across classes by default,
although the more general function \texttt{tidySEM::mx\_mixture()}
could be used to circumvent this.
After all models have been estimated,
the function \texttt{tidySEM::table\_fit()} can be used
to obtain a model fit table suitable for
determining the optimal model according to best practices.
Note, however, that this table does not include
the bootstrapped likelihood ratio test (BLRT) by default,
because this test is very computationally expensive.
It is recommended to use the function \texttt{tidySEM::BLRT()}
to compare a shortlist of likely candidate models
based on other fit indices.
More on fit indices can be found in
the Model Fit Indices subsection of this paper.

Confirmatory LCA typically requires
less comprehensive alternative model specifications.
For example, in the context of a preregistered analysis,
the main models of interest would have been specified a priori.
Even in this case,
the theoretical model could be compared to a few others to contextualize it.

\hypertarget{software}{%
\subsubsection{Software}\label{software}}

Many software packages are available for the estimation of LCA models.
Some of these packages have limited functionality,
or implement specific innovations.
Other packages implement LCA in the context of
a more flexible structural equation modeling framework.
The most notable examples of the latter are
the commercial programs Mplus and Latent GOLD,
and the free open source R-package OpenMx.
The commercial packages stand out
because they offer relatively user-friendly interfaces
and implement sensible defaults for complex analyses,
including LCA methods.
This lowers the threshold for applied researchers to adopt such methods.
Commercial software also has several downsides, however.
One such downside is that use of the software
is restricted to those individuals
and institutions who can afford a license.
Another downside is that the source code,
being proprietary, cannot be audited, debugged,
or enhanced by third parties.
This incurs the risk that mistakes in the source code
may go unnoticed, and curbs progress as
software developers cannot add new functionality.

Conversely, the free open source program OpenMx is very flexible,
but not very user-friendly.
We directly address this limitation using the \texttt{tidySEM} R-package.
New functionality in \texttt{tidySEM} seeks to lower the threshold
for latent class analysis using \texttt{OpenMx}.
It adheres to best practices in estimation and reporting,
as described in this paper.
The user interface is simple,
making use of the model syntax of the widely used \texttt{lavaan} R-package.
This syntax offers a human-readable way to specify latent variable models.
Minor enhancements are made to simplify the specification of LCA.

Because of the limitations of the aforementioned tools,
we set out to develop a free tool that
provides sensible defaults and is easy to use,
but provides the option to access and modify
all of the model inputs (i.e., low barrier, high ceiling).
\texttt{tidySEM} interfaces with existing tools,
and is able to translate between what existing tools are capable of
and what researchers and analysts
carrying-out person-oriented analyses would like to specify.
Furthermore,\texttt{tidySEM} facilitates fully-reproducible analyses
and contributes to open science.

\hypertarget{algorithm}{%
\subsubsection{Algorithm}\label{algorithm}}

LCA parameters and model fit statistics
can be estimated in a variety of ways.
The choice of the estimator
depends on the presence of missing values,
sample size, number of indicators,
and available computational resources (Weller, Bowen, \& Faubert, 2020).
A commonly used technique is maximum likelihood (ML) estimation
with the expectation-maximization (EM) algorithm as a local optimizer.
Imagine we are estimating two parameters,
e.g.~the class-specific means \(\mu_c\) on a continuous indicator
(ignoring the variance for now).
The EM algorithm will attempt to find a combination of values
for these two parameters that maximizes
the likelihood (\(LL\)) of all observed data.
In practice, instead of maximizing \(LL\),
often \(-2*LL\) is minimized,
as this offers computational advantages.
We can think of this optimization problem
as a three-dimensional landscape:
The X and Y dimensions are determined by the class-specific means,
so \(X = \mu_1\) and \(Y = \mu_2\) -
and the Z-dimension is determined by \(Z = -2*LL\).
The optimizer must find the deepest ``valley'' in this landscape,
which reflects the combination of \(\mu_1\) and \(\mu_2\) that
maximizes the likelihood of the data.
The EM optimizer behaves somewhat like a marble dropped in this landscape.
It is dropped at some random point in space,
and will roll into the nearest valley.
The problem is that, once EM rolls into a valley,
it will settle on the bottom of that valley
(this is known as ``convergence'').
It cannot climb out again.
Thus, if their are multiple valleys,
the risk is that the optimizer gets stuck in a shallower valley (a ``local optimum''),
and never discovers the deepest valley (the ``global optimum'', or best solution).
One solution to this problem is to drop many marbles at random places,
compare their final \(-2*LL\) values,
choose the solution with the lowest \(-2*LL\),
and make sure that several marbles replicated this solution.
This is the ``random starts'' approach.

One problem with the random starts approach is that
it is computationally expensive to run this many replications.
Moreover, because the algorithm begins with random starting values,
many of the marbles are likely to be very far away from a ``good enough'' solution.
Two innovations may improve the estimation procedure.
The first is that, instead of picking random starting values,
a ``reasonable solution'' may be used for the starting values.
For example, if we assume that the different classes are likely to have
different mean values on the indicators,
then the K-means clustering algorithm can be used to
determine these cluster centroids.
We can compute the expected values of all model parameters
by treating the K-means solution as a known class solution,
and use these as starting values for a mixture model.
One remaining concern is that this approach may result
in starting values close to a local optimum,
and that the EM algorithm will thus never find the global optimum.
A second innovation addresses this concern.
Instead of using EM,
it is possible to use an optimizer that can climb out of a valley.
Simulated annealing iteratively considers some ``destination'' in the landscape,
and compares its likelihood to the current one.
If the destination likelihood is higher,
the estimator moves there.
If the destination likelihood is \emph{lower},
the estimator still moves there occasionally,
based on probability.
This latter property allows it to escape local optima,
and find the global optimum.

By default, \texttt{tidySEM} derives starting values using K-means clustering,
and identifies the global optimum solution using simulated annealing.
Once a solution has been found,
simulated annealing is followed up with a short run of the EM algorithm,
as EM inherently produces an asymptotic covariance matrix for
the parameters that can be used to compute standard errors.
Note that these defaults can be manually overridden.

A recent paper suggested maximum likelihood with robust standard errors
should be used when the observed indicators are not normally distributed (Spurk et al., 2020).
This statement is incorrect, and may lead readers to believe that
they must use commercial software,
as robust maximum likelihood is currently only implemented
in Mplus and latentGOLD.
As explained before,
mixture modeling assumes that observed data are
a mixture of (multivariate) normal distributions;
thus, the observed indicators will likely not be normally distributed.

\hypertarget{class-enumeration}{%
\subsubsection{Class Enumeration}\label{class-enumeration}}

As explained, LCA can be done in an exploratory or in a confirmatory fashion.
In exploratory LCA, a sequence of models is fitted to the data
with each additional model estimating one more class than the previous model.
These models are then compared
and the best solution is selected as the final class solution.
In some cases, prior theory can inform the researcher
about the number of classes to expect.
Even in such confirmatory LCA cases,
it is nonetheless useful to know if the theoretical model
is markedly better than those with differing numbers of classes.
Therefore, it may always be useful to compare different class solutions.

From a sequence of models, the final class solution is chosen
based on both theoretical and statistical criteria.
Theory should drive the selection of indicator variables,
inform the expectations and reflect on the findings.
In addition to this, there are several statistical criteria
to consider in model selection.
These include but are not limited to likelihood ratio tests,
information criteria,
and the Bayes factor (Weller et al., 2020).

Relative model fit can be examined using the likelihood ratio test.
This is only appropriate when the two models we wish to compare are nested.
The likelihood ratio test statistic is computed as
the difference in maximum log likelihoods of the two models,
with the test degrees of freedom being
the difference in the degrees of freedom of the two compared models.
The test statistic follows the \(\chi^2\) distribution,
and we want it to be non-significant
in order to give support to the simpler model.
The likelihood ratio test can only compare two nested models at a time (Lanza, Bray, \& Collins, 2003).

\hypertarget{model-fit-indices}{%
\subsubsection{Model Fit Indices}\label{model-fit-indices}}

Fit indices typically used for determining the optimal number of classes include
the Akaike Information Criterion (AIC)
and the Bayesian Information Criterion (BIC).
Both information criteria are based on the -2*log likelihood
(which is lower for better fitting models),
and add a penalty for the number of parameters (thus incentivizing simpler models).
This helps balance model fit and model complexity.
The lower the value of an information criterion,
the better the overall fit of the model.
The BIC applies a stronger penalty for model complexity
that scales logarithmically with the sample size.
The literature suggests the BIC may be
the most appropriate information criterion to use for model comparison (Nylund-Gibson \& Choi, 2018).
Both the AIC and the BIC are available in the \texttt{tidySEM} output.

Information criteria may occasionally contradict each other,
so it is important to identify a suitable strategy to reconcile them.
One option is to select a specific fit index before analyzing the data.
Another option is to always prefer
the most parsimonious model that has best fit
according to any of the available fit indices.
Yet another option is to incorporate information from
multiple fit indices using the analytic hierarchy process
(Akogul \& Erisoglu, 2016).
Finally, one might make an elbow plot and
compare multiple information criteria
(Nylund-Gibson \& Choi, 2018).

Another common test of model fit is
the likelihood ratio \(\chi^2\) goodness-of-fit test.
However, this test is not implemented in \texttt{tidySEM}.

LCA studies commonly report -2*log likelihood of the final class solution.
This is a basic fit measure used to compute most information criteria.
However, since log likelihood is not penalized for model complexity,
it will continuously fall with the addition of more classes.

An alternative is using the bootstrapped likelihood ratio test
which can be run using \texttt{tidySEM::BLRT()}.
Currently, this test is computationally expensive and
can be slow on most computers.
A faster version of this test,
namely an implementation of \emph{the lazy bootstrap} (Kollenburg, Mulder, \& Vermunt, 2018)
to \texttt{tidySEM} is being developed.

\hypertarget{classification-diagnostics}{%
\subsubsection{Classification Diagnostics}\label{classification-diagnostics}}

Best models will divide the sample into subgroups
which are internally homogeneous and externally distinct.
Classification diagnostics give us a way to assess
the degree to which this is the case.
They are separate from model fit indices as
a model can fit the data well
but show poor latent class separation (Masyn, 2013).
Classification diagnostics should not be used for model selection,
but they can be used to disqualify certain solutions
because they are uninterpretable.
Interpretability should always be a consideration
when considering different class solutions (Nylund-Gibson \& Choi, 2018).

Three important classification diagnostics provided by \texttt{tidySEM} are:
(1) \emph{the minimum} and \emph{maximum percentage of the sample assigned to a particular class},
(2) \emph{the range of the posterior class probabilities by most likely class membership},
and (3) \emph{entropy}.
All three are based on posterior class probabilities.

The posterior class probability is a measure of classification uncertainty
which can be computed for each individual,
or averaged for each latent class.
When the posterior class probability is computed for each individual in the dataset,
it represents each person's probability of belonging to each latent class.
For each person, the highest posterior class probability is then determined
and the individual is assigned to the corresponding class.
We want each individual's posterior class probabilities to be
high for one and low for the remaining latent classes.
This is considered a high classification accuracy
and means that the classes are distinct.
To obtain posterior class probabilities, run \texttt{tidySEM::class\_prob()}.
This function produces output comprised of several elements:

\texttt{\$sum.posterior} is a summary table of the posterior class probabilities
indicating what proportion of the data contributes to each class.

\texttt{\$sum.mostlikely} is a summary table of the most likely class membership
based on the highest posterior class probability.
From this table, we compute the minimum and maximum
percentage of the sample assigned to a particular class,
, i.e.~\textbf{n\_min}
(the smallest class proportion based on the posterior class probabilities)
and \textbf{n\_max}
(the largest class proportion based on the posterior class probabilities).
We are especially interested in \textbf{n\_min} as
if it is very small and comprised of few observations,
the model for that group might not be locally identified.
It may be impossible to calculate descriptive statistics for such a small class.
Estimating LCA parameters on small subsamples
might lead to bias in the results.
Therefore, we advise caution when dealing with small classes.

\texttt{\$mostlikely.class} is a table with rows representing
the class the person was assigned to,
and the columns indicating the average posterior probability.
The diagonal represents the probability that
observations in each class will be correctly classified.
If any of the values on the diagonal of this table is low,
we might consider not to interpret that solution.
In \texttt{tidySEM} we use the diagonal to compute the range of
the posterior class probabilities by most likely class membership
which consists of
the lowest class posterior probability (\textbf{prob\_min}),
and the highest posterior probability (\textbf{prob\_max}).
Both \textbf{prob\_min} and \textbf{prob\_max} can be used to
disqualify certain class solutions,
and are a convenient way to summarize class separation in LCA.
We want both \textbf{prob\_min} and \textbf{prob\_max} to be high as
that means that for all classes
the people who were assigned to that class
have a high probability of being there.
\textbf{prob\_min} is especially important as it can diagnose
if there is a class with low posterior probabilities
which could make one reconsider that class solution.

\texttt{\$avg.mostlikely} contains the average posterior probabilities for each class,
for the subset of observations with most likely class of 1:k,
where k is the number of classes.

\texttt{\$individual} is the individual posterior probability matrix,
with dimensions n (number of cases in the data) x k (number of classes).
Individual class probabilities are often useful for researchers
who wish to do follow up analyses.

Entropy is a summary measure of posterior class probabilities
across classes and individuals.
It ranges from 0 (model classification no better than random chance)
to 1 (perfect classification).
As a rule of thumb, values above .80 are deemed acceptable
and those approaching 1 are considered ideal.
An appropriate use of entropy is that it can disqualify certain solutions
if class separability is too low.
Entropy was not built for nor should it be used for
model selection during class enumeration (Masyn, 2013).

\textbf{n\_min}, \textbf{n\_max}, \textbf{prob\_min}, \textbf{prob\_max}, and \textbf{entropy} and can be obtained using \texttt{tidySEM::table\_fit()}.

\hypertarget{interpreting-class-solutions}{%
\subsubsection{Interpreting Class Solutions}\label{interpreting-class-solutions}}

An important outcome of LCA are conditional item probabilities,
also known as class-specific item probabilities (Masyn, 2013),
conditional response, or conditional solution probabilities (Geiser, 2012).
They indicate the probability of an item being endorsed
given that the observation belongs to a particular latent class.
Conditional item probabilities can be obtained using \texttt{tidySEM::table\_prob()}.
If a particular item is endorsed by two or more classes at markedly different rates,
it is said to discriminate well between the classes
and is consequently considered a good indicator.
Classes are considered highly homogeneous with respect to an item
when for a particular item
there is a distinct difference in conditional item probabilities
for two or more classes.
For instance, if an item is endorsed below 30\% for one class and
above 70\% for another class,
the classes have high homogeneity with respect to this item (Masyn, 2013).
Conditional item probabilities are the analogue of mean and standard deviation
when the indicators are binary or ordinal.

A problem which can occur is that of inadmissible solutions.
With binary indicators, LCA is modelling a cross-table with all the predictors.
The problem with such cross-tables is that they will often contain empty cells,
i.e.~combinations of responses that never occur together.
This problem is reflected by extreme conditional item probabilities (as in exactly 0 or 1).
Such boundary parameter estimates could indicate that the solution is invalid (Geiser, 2012).
Boundary parameter estimates can also happen with continuous indicators.
For instance, if we have a zero-inflated normal distribution and a two class solution,
one class might have the mean of zero and its standard deviation cannot be determined
since there is little variance.
This too could be a sign of an invalid solution,
warn that too many classes were extracted, or indicate a local optimum (Geiser, 2012).

\hypertarget{label-switching}{%
\subsubsection{Label Switching}\label{label-switching}}

The final class solution will usually discover and enumerate several classes.
The class ordering, however, is completely arbitrary.
The class labeled as Class 1 in one solution
may become Class 2 or Class 3 in another model,
even when the only difference between the models is in their starting values.
Label switching is something to be mindful of when comparing different LCA models (Masyn, 2013).

The order of clusters is nondeterministic when using K-means in \texttt{tidySEM}.
Therefore label switching is still a consideration.
A simple solution to this is setting a random seed number one line prior to fitting the model.
We advise \texttt{tidySEM} users to always do so in order to circumvent label switching.

Class names should be chosen to accurately reflect group membership.
Overly simplified and generalized class names may prove misleading
to both audiences and researches alike
leading to what is known as a naming fallacy (Weller et al., 2020).

\hypertarget{best-practices-in-reporting}{%
\subsection{Best Practices in Reporting}\label{best-practices-in-reporting}}

Among studies using LCA, reporting practices vary significantly (Weller et al., 2020).
Various authors have tried to improve and standardize ways of reporting LCA
(e.g. Masyn, 2013; Weller et al., 2020),
but more work is needed.
Van Lissa et al. (2020) developed WORCS,
a workflow for open reproducible code in science.
WORCS consists of step-by-step guidelines for research projects
based on the TOP-guidelines developed by Nosek et al. (2015).
WORCS workflow can be easily implemented in R in form of
an R package which facilitates preregistration, article drafting,
version control, citation and formatting, among others (Van Lissa et al., 2020).

TOP-guidelines emphasise the use of comprehensive citation
(including referencing the software used in the analysis),
as well as code and data sharing wherever possible (Nosek et al., 2015).
Van Lissa et al. (2020) suggest sharing synthetic data in case
the original data cannot be shared,
and provide functions to generate such synthetic data.
Ideally, the entire research project is made reproducible
so that others may download it and reproduce it with just one click;
for guidance, see Van Lissa et al. (2020).

As the open science movement is gaining momentum,
researchers are becoming increasingly aware
how important it is that analyses can be reproduced and audited.
In line with open science principles,
one of the suggested reporting standards relates to reproducible code.
In this context, it is important to note that
user-friendly methods for estimating LCA models
have predominantly been available in commercial software packages
(e.g., \emph{Mplus} and \emph{Latent GOLD}).
A potential downside of commercial software is that
it restricts the ability to reproduce analyses to license holders,
and prevents auditing research because
the underlying source code is proprietary.
To overcome these limitations,
the present paper introduces new user-friendly functions in
the \texttt{tidySEM} R-package that can be used to
estimate a wide range of LCA models
using the free, open-source R-package \texttt{OpenMx}.
The reporting guidelines described in this paper
are adopted in \texttt{tidySEM} by default.
The \texttt{tidySEM} R-package thus makes advanced mixture modeling
based on best practices widely accessible,
and facilitates the adoption of the estimation and
reporting guidelines described in this paper.

\hypertarget{visualization}{%
\subsubsection{Visualization}\label{visualization}}

Plots can greatly improve the interpretability of LCA models.
There are several stages where this is an important consideration.

First, in class enumeration,
we want to compare several competing class solutions.
This can be done by means of the AIC and BIC.
Here, we suggest using an elbow plot
with the \(K-class\ solution\) on the X-axis,
and information criteria value on the Y-axis.
We show an implementation of such an elbow plot
in our Tutorial subsection.

Second, once we have decided on the final class solution,
we want to interpret the response patterns
on the indicators for each latent class.
While \texttt{tidySEM} can create a table showing
the probability of each item's response endorsement
(using \texttt{tidySEM::table\_prob}),
it may be easier to inspect these probabilities visually.
For this reason, we created \texttt{ggplot2::plot\_prob()}.
The resulting shows response patterns
on all indicators for each group.
We give an example of this plot in the Tutorial subsection.

\hypertarget{tutorial}{%
\section{Tutorial}\label{tutorial}}

This is an example of an exploratory LCA using \texttt{tidySEM}.

\hypertarget{loading-the-dataset}{%
\subsection{Loading the Dataset}\label{loading-the-dataset}}

In this example, we use \emph{data\_mix\_ordinal},
a simulated data for mixture modelling with ordinal indicators.
This dataset is built-in to \texttt{tidySEM}.
For more information about the dataset,
type \texttt{?tidySEM::data\_mix\_ordinal} into the R console
after loading the \texttt{tidySEM} using library().

We load the dataset and convert the indicators into ordered factors.

\hypertarget{exploring-the-data}{%
\subsection{Exploring the Data}\label{exploring-the-data}}

An important step to preceding any statistical analysis is data exploration.
Here we use \texttt{tidySEM::descriptives()}
to describe the dataset we are using.

As we can see, the output includes various descriptives of our dataset.
Special attention should be paid to examining the pattern of missingness,
as discussed in the \emph{Handling missing data} section of this paper.
In our example, we see that there are no missing values,
hence we proceed with our analysis.

\hypertarget{conducting-latent-class-analysis}{%
\subsection{Conducting Latent Class Analysis}\label{conducting-latent-class-analysis}}

Before we fit a series of LCA models, we set a random seed
using \texttt{set.seed()}.
The seed number can be any digit.
This is an important step as
there is some inherent randomness in the LCA computations,
and having the same seed number ensures that
two separate researcher obtain exactly the same results
when fitting LCA models.

Finally, we reach the step of fitting LCA models.
To do so, we use \texttt{tidySEM::mx\_lca()}
which takes data and number of classes as inputs.
In our example, we want to fit 1 to 4 class solutions
and compare their output.
Depending on your computer's computational power,
this might take a while.

\hypertarget{class-enumeration-1}{%
\subsection{Class Enumeration}\label{class-enumeration-1}}

In class enumeration,
we want to compare a sequence of LCA models fitted to the data.
To aid the process, we create a model fit table using \texttt{tidySEM::table\_fit()}
with the results object as the input.
As the output contains a lot of information on each of the four fitted models,
we select a subset of helpful model fit indices and classification diagnostics.

Our selection of fit indices and classification diagnostics includes:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1220}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8780}}@{}}
\caption{\label{tab:unnamed-chunk-6}Selection of Fit Indices and Classification Diagnostics}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Selection
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Selection
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule()
\endhead
Name & the \(K\)-class solution \\
LL & the -2*log-likelihood of each model \\
AIC & the Akaike Information Criterion value \\
BIC & the Bayesian Information Criterion value \\
prob\_min & the lowest posterior class probability by most likely class membership \\
prob\_max & the highest posterior class probability by most likely class membership \\
n\_min & the lowest class proportion based on the posterior class probabilities \\
n\_max & the highest class proportion based on the posterior class probabilities \\
\bottomrule()
\end{longtable}

We discussed several possible strategies to select the final class solution.
Here, we apply our own.

To aid our interpretation of the results,
we create an elbow plot showing the trends in information criteria across four models.

From the elbow plot, we see that AIC has a lower penalty for model complexity.
However, we are more interested in the BIC values, which are similar
for the one, two and three-class solutions, but the four-class solution fits significantly worse.
For this reason, we eliminate the four-class solution from the selection process.

Then we examine the model fit table.
As expected, the -2*log likelihood falls successively with each added class.
As previously stated, classification diagnostics should not be used for model selection,
but they can be used to disqualify certain solutions because they are uninterpretable.
We see that prob\_min for the four-class solution is low,
knowing that this solution also has a high BIC, we disqualify this solution.

Out of the remaining three solutions,
we notice that entropy is the highest for the three-class solution,
and it has a satisfactory prob\_min and n\_min.
Based on this, we retain the three class solution in model selection.
Note that entropy for the one-class solution will always equal to one,
as it is 100\% true that every case is in that class.
Based on the low entropy of the two-class solution,
we eliminate this model.

Finally, when comparing the one and three-class solutions,
we inspect the information criteria.
For BIC, the one-class solution fits better, but the difference is marginal.
AIC tells us that the added complexity
of having three classes still explains the data better
than a one-class solution.
Therefore, we select the three-class solution as our final-class solution.

\hypertarget{interpreting-the-final-class-solution}{%
\subsection{Interpreting the Final Class Solution}\label{interpreting-the-final-class-solution}}

To aid our understanding of the final class solution,
we use \texttt{ggplot2::plot\_prob()}
with the results of the three-class model as the input.
The resulting graph shows response patterns
on all the indicators for each group.

If we want to know the probability of each response option's endorsement
for each class, we can use \texttt{tidySEM::table\_prob}.
These are thresholds for ordinal dependent variables in the probability scale.

In the plot, we can see the distributions of the response probabilities on the indicators
for each of the three classes.
For instance, we see that in Class 1 the most common response to u2 is 2,
while in Class 2 and Class 3 this is 0.
We can also see that response 1 is a rare response not forming the majority in
any class.
Class 2 distinguishes itself because the majority scores the response 0 category of u3 and u4,
while in Class 1 and 2 this is not the case.
Class 3 distinguishes itself because the most common response to u3 and u4 is 3.

We can also interpret the response patterns numerically.

\hypertarget{extracting-posterior-class-probabilities}{%
\subsection{Extracting Posterior Class Probabilities}\label{extracting-posterior-class-probabilities}}

Another step is to extract posterior class probabilities.
This is done by the use of \texttt{TidySEM::class\_prob}
with the results of the final class solution as the input.

\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-akogul_comparison_2016}{}}%
Akogul, S., \& Erisoglu, M. (2016). A {Comparison} of {Information} {Criteria} in {Clustering} {Based} on {Mixture} of {Multivariate} {Normal} {Distributions}. \emph{Mathematical and Computational Applications}, \emph{21}(3), 34. \url{https://doi.org/10.3390/mca21030034}

\leavevmode\vadjust pre{\hypertarget{ref-baughman_mixture_2006}{}}%
Baughman, A. L., Bisgard, K. M., Lynn, F., \& Meade, B. D. (2006). Mixture model analysis for establishing a diagnostic cut-off point for pertussis antibody levels. \emph{Statistics in Medicine}, \emph{25}(17), 2994--3010. \url{https://doi.org/10.1002/sim.2442}

\leavevmode\vadjust pre{\hypertarget{ref-figueiredo_unsupervised_2002}{}}%
Figueiredo, M. A. T., \& Jain, A. K. (2002). Unsupervised learning of finite mixture models. \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, \emph{24}(3), 381--396. \url{https://doi.org/10.1109/34.990138}

\leavevmode\vadjust pre{\hypertarget{ref-geiser_data_2012}{}}%
Geiser, C. (2012). \emph{Data {Analysis} with {Mplus}}. Guilford Press.

\leavevmode\vadjust pre{\hypertarget{ref-geiser_is_2014}{}}%
Geiser, C., \& Wurpts, I. (2014). Is adding more indicators to a latent class analysis beneficial or detrimental? {Results} of a {Monte} {Carlo} study. \emph{Frontiers in Psychology: Quantitative Psychology and Measurement}, \emph{5}. https://doi.org/\url{https://doi.org/10.3389/fpsyg.2014.00920}

\leavevmode\vadjust pre{\hypertarget{ref-giang_using_2008}{}}%
Giang, M. T., \& Graham, S. (2008). Using latent class analysis to identify aggressors and victims of peer harassment. \emph{Aggressive Behavior}, \emph{34}(2), 203--213. \url{https://doi.org/10.1002/ab.20233}

\leavevmode\vadjust pre{\hypertarget{ref-hennig_handbook_2015}{}}%
Hennig, C., Meila, M., Murtagh, F., \& Rocci, R. (2015). \emph{Handbook of {Cluster} {Analysis}}. 28.

\leavevmode\vadjust pre{\hypertarget{ref-hopfer_social_2014}{}}%
Hopfer, S., Tan, X., \& Wylie, J. L. (2014). A {Social} {Network}--{Informed} {Latent} {Class} {Analysis} of {Patterns} of {Substance} {Use}, {Sexual} {Behavior}, and {Mental} {Health}: {Social} {Network} {Study} {III}, {Winnipeg}, {Manitoba}, {Canada}. \emph{American Journal of Public Health}, \emph{104}(5), 834--839. \url{https://doi.org/10.2105/AJPH.2013.301833}

\leavevmode\vadjust pre{\hypertarget{ref-jamshidian_tests_2010}{}}%
Jamshidian, M., \& Jalal, S. (2010). Tests of {Homoscedasticity}, {Normality}, and {Missing} {Completely} at {Random} for {Incomplete} {Multivariate} {Data}. \emph{Psychometrika}, \emph{75}(4), 649--674. \url{https://doi.org/10.1007/s11336-010-9175-3}

\leavevmode\vadjust pre{\hypertarget{ref-van_kollenburg_lazy_2018}{}}%
Kollenburg, G. H. van, Mulder, J., \& Vermunt, J. K. (2018). \emph{The {Lazy} {Bootstrap}. {A} {Fast} {Resampling} {Method} for {Evaluating} {Latent} {Class} {Model} {Fit}}. 23.

\leavevmode\vadjust pre{\hypertarget{ref-lanza_introduction_2003}{}}%
Lanza, S. T., Bray, B. C., \& Collins, L. M. (2003). An {Introduction} to {Latent} {Class} and {Latent} {Transition} {Analysis}. In \emph{Handbook of {Psychology}: {Research} {Methods} in {Psychology}} (2nd ed., Vol. 2, pp. 690--712). John Wiley \& Sons.

\leavevmode\vadjust pre{\hypertarget{ref-lanza_latent_2013}{}}%
Lanza, S. T., Tan, X., \& Bray, B. C. (2013). Latent {Class} {Analysis} {With} {Distal} {Outcomes}: {A} {Flexible} {Model}-{Based} {Approach}. \emph{Structural Equation Modeling : A Multidisciplinary Journal}, \emph{20}(1), 1--26. \url{https://doi.org/10.1080/10705511.2013.742377}

\leavevmode\vadjust pre{\hypertarget{ref-lee_comparison_2021}{}}%
Lee, T., \& Shi, D. (2021). A comparison of full information maximum likelihood and multiple imputation in structural equation modeling with missing data. \emph{Psychological Methods}, No Pagination Specified--No Pagination Specified. \url{https://doi.org/10.1037/met0000381}

\leavevmode\vadjust pre{\hypertarget{ref-little_test_1988}{}}%
Little, R. J. A. (1988). A {Test} of {Missing} {Completely} at {Random} for {Multivariate} {Data} with {Missing} {Values}. \emph{Journal of the American Statistical Association}, \emph{83}(404), pp. 1198--1202. \url{https://doi.org/10.2307/2290157}

\leavevmode\vadjust pre{\hypertarget{ref-macgregor_symptom_2021}{}}%
MacGregor, A. J., Dougherty, A. L., D'Souza, E. W., McCabe, C. T., Crouch, D. J., Zouris, J. M., \ldots{} Fraser, J. J. (2021). Symptom profiles following combat injury and long-term quality of life: A latent class analysis. \emph{Quality of Life Research}, \emph{30}(9), 2531--2540. \url{https://doi.org/10.1007/s11136-021-02836-y}

\leavevmode\vadjust pre{\hypertarget{ref-masyn_latent_2013}{}}%
Masyn, K. E. (2013). Latent {Class} {Analysis} and {Finite} {Mixture} {Modeling}. In \emph{The {Oxford} {Handbook} of {Quantitative} {Methods}}: \emph{Vol.} \emph{2: Statistical Analysis} (p. 551). Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-norman_likert_2010}{}}%
Norman, G. (2010). Likert scales, levels of measurement and the {``laws''} of statistics. \emph{Advances in Health Sciences Education}, \emph{15}(5), 625--632. \url{https://doi.org/10.1007/s10459-010-9222-y}

\leavevmode\vadjust pre{\hypertarget{ref-nosek_promoting_2015}{}}%
Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., \ldots{} Yarkoni, T. (2015). Promoting an open research culture. \emph{Science}, \emph{348}(6242), 1422--1425. \url{https://doi.org/10.1126/science.aab2374}

\leavevmode\vadjust pre{\hypertarget{ref-nozadi_moderating_2016}{}}%
Nozadi, S. S., Troller-Renfree, S., White, L. K., Frenkel, T., Degnan, K. A., Bar-Haim, Y., \ldots{} Fox, N. A. (2016). The {Moderating} {Role} of {Attention} {Biases} in understanding the link between {Behavioral} {Inhibition} and {Anxiety}. \emph{Journal of Experimental Psychopathology}, \emph{7}(3), 451--465. \url{https://doi.org/10.5127/jep.052515}

\leavevmode\vadjust pre{\hypertarget{ref-nylund_deciding_2007}{}}%
Nylund, K. L., Asparouhov, T., \& MuthÃ©n, B. O. (2007). Deciding on the {Number} of {Classes} in {Latent} {Class} {Analysis} and {Growth} {Mixture} {Modeling}: {A} {Monte} {Carlo} {Simulation} {Study}. \emph{Structural Equation Modeling: A Multidisciplinary Journal}, \emph{14}(4), 535--569. \url{https://doi.org/10.1080/10705510701575396}

\leavevmode\vadjust pre{\hypertarget{ref-nylund-gibson_ten_2018}{}}%
Nylund-Gibson, K., \& Choi, A. Y. (2018). Ten frequently asked questions about latent class analysis. \emph{Translational Issues in Psychological Science}, \emph{4}(4), 440--461. \url{https://doi.org/10.1037/tps0000176}

\leavevmode\vadjust pre{\hypertarget{ref-rubin_inference_1976}{}}%
Rubin, D. B. (1976). Inference and {Missing} {Data}. \emph{Biometrika}, \emph{63}(3), 581--592. \url{https://doi.org/10.2307/2335739}

\leavevmode\vadjust pre{\hypertarget{ref-scrucca_mclust_2016}{}}%
Scrucca, L., Fop, M., Murphy, T. B., \& Raftery, A. E. (2016). \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096736}{Mclust 5: {Clustering}, {Classification} and {Density} {Estimation} {Using} {Gaussian} {Finite} {Mixture} {Models}}. \emph{The R Journal}, \emph{8}(1), 289--317.

\leavevmode\vadjust pre{\hypertarget{ref-spurk_latent_2020}{}}%
Spurk, D., Hirschi, A., Wang, M., Valero, D., \& Kauffeld, S. (2020). Latent profile analysis: {A} review and {``how to''} guide of its application within vocational behavior research. \emph{Journal of Vocational Behavior}, \emph{120}, 103445. \url{https://doi.org/10.1016/j.jvb.2020.103445}

\leavevmode\vadjust pre{\hypertarget{ref-van_de_schoot_grolts-checklist_2017}{}}%
Van De Schoot, R., Sijbrandij, M., Winter, S. D., Depaoli, S., \& Vermunt, J. K. (2017). The {GRoLTS}-{Checklist}: {Guidelines} for {Reporting} on {Latent} {Trajectory} {Studies}. \emph{Structural Equation Modeling: A Multidisciplinary Journal}, \emph{24}(3), 451--467. \url{https://doi.org/10.1080/10705511.2016.1247646}

\leavevmode\vadjust pre{\hypertarget{ref-van_lissa_worcs_2020}{}}%
Van Lissa, C. J., Brandmaier, A. M., Brinkman, L., Lamprecht, A.-L., Peikert, A., Struiksma, M. E., \& Vreede, B. (2020). \emph{{WORCS}: {A} {Workflow} for {Open} {Reproducible} {Code} in {Science}}. \url{https://doi.org/10.17605/OSF.IO/ZCVBS}

\leavevmode\vadjust pre{\hypertarget{ref-van_lissa_worcs_2021}{}}%
Van Lissa, C. J., Brandmaier, A. M., Brinkman, L., Lamprecht, A.-L., Peikert, A., Struiksma, M. E., \& Vreede, B. M. I. (2021). {WORCS}: {A} workflow for open reproducible code in science. \emph{Data Science}, \emph{4}(1), 29--49. \url{https://doi.org/10.3233/DS-210031}

\leavevmode\vadjust pre{\hypertarget{ref-vermunt_k-means_2011}{}}%
Vermunt, J. K. (2011). K-means may perform as well as mixture model clustering but may also be much worse: {Comment} on {Steinley} and {Brusco} (2011). \emph{Psychological Methods}, \emph{16}(1), 82--88. \url{https://doi.org/10.1037/a0020144}

\leavevmode\vadjust pre{\hypertarget{ref-vermunt_latent_2017}{}}%
Vermunt, J. K. (2017). Latent {Class} {Modeling} with {Covariates}: {Two} {Improved} {Three}-{Step} {Approaches}. \emph{Political Analysis}, \emph{18}(4), 450--469. \url{https://doi.org/10.1093/pan/mpq025}

\leavevmode\vadjust pre{\hypertarget{ref-vermunt_jk_latent_2004}{}}%
Vermunt, J.K., Magidson, J., Lewis-Beck, M., Bryman, A., Liao, T.F., \& Department of Methodology and Statistics. (2004). Latent class analysis. In \emph{The {Sage} encyclopedia of social sciences research methods} (pp. 549--553). Sage. Retrieved from \url{https://research.tilburguniversity.edu/en/publications/0caedd00-27c1-42bd-bb4e-d1dcb0864956}

\leavevmode\vadjust pre{\hypertarget{ref-weller_latent_2020}{}}%
Weller, B. E., Bowen, N. K., \& Faubert, S. J. (2020). Latent {Class} {Analysis}: {A} {Guide} to {Best} {Practice}. \emph{Journal of Black Psychology}, \emph{46}(4), 287--311. \url{https://doi.org/10.1177/0095798420930932}

\leavevmode\vadjust pre{\hypertarget{ref-wu_abuse_2011}{}}%
Wu, L.-T., Woody, G. E., Yang, C., Pan, J.-J., \& Blazer, D. G. (2011). Abuse and dependence on prescription opioids in adults: A mixture categorical and dimensional approach to diagnostic classification. \emph{Psychological Medicine}, \emph{41}(3), 653--664. \url{https://doi.org/10.1017/S0033291710000954}

\end{CSLReferences}


\end{document}
