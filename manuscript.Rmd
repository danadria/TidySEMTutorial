---
title             : "Best Practices in Latent Class Analysis using Free Open Source Software"
shorttitle        : "BEST PRACTICES LATENT CLASS ANALYSIS"
author: 
  - name          : "Caspar J. van Lissa"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Padualaan 14, 3584CH Utrecht, The Netherlands"
    email         : "c.j.vanlissa@gmail.com"
  - name          : "Mauricio Garnier-Villarreal"
    affiliation   : "3"
    corresponding : no   
    email         : "m.garniervillarreal@vu.nl"
  - name          : "Daniel Anadria"
    affiliation   : "1"
    corresponding : no
    address       : "Padualaan 14, 3584CH Utrecht, The Netherlands"
    email         : "d.anadria@uu.nl"
affiliation:
  - id            : "1"
    institution   : "Utrecht University, Methodology & Statistics"
  - id            : "2"
    institution   : "Open Science Community Utrecht"
  - id            : "3"
    institution   : "Vrije Universiteit Amsterdam, Sociology"
author_note: |
  Methodology & Statistics
  Padualaan 14
  3582CH Utrecht, The Netherlands
abstract: |
  Latent class analysis refers to a family of techniques for identifying groups
  in data based on a parametric model.
  Examples include mixture models, latent class analysis with ordinal
  indicators, and latent class growth analysis.
  Despite the popularity of this technique, there is limited
  guidance with respect to best practices in estimating and reporting mixture
  models. Moreover, although user-friendly interfaces for advanced mixture
  modeling have long been available in commercial software packages, open
  source alternatives have remained somewhat inaccessible. This tutorial
  describes best practices for the estimation and reporting of latent class analysis,
  using free and open source software in R. To this end, this tutorial
  introduces new functionality for estimating and reporting mixture
  models in the `tidySEM` R-package whose backend relies on `OpenMx`.
keywords          : "latent class analysis, mixture models, best practices, free open source software, tidySEM"
wordcount         : "9001"
bibliography      : "mixture.bib"
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
class             : "man"
output            : papaja::apa6_pdf
editor_options: 
  markdown: 
    wrap: 72
---

```{r load_packages, include = FALSE}
library("papaja")
#bibliography      : ["r-references.bib"]
library(worcs)
run_everything = FALSE
```

Latent class analysis (LCA) is an umbrella term that refers to a number
of techniques for estimating unobserved group membership based on a
parametric model of one or more observed indicators of group membership.
The types of LCA have become quite popular across scientific fields,
most notably finite Gaussian mixture modeling and latent profile
analysis. @vermunt_latent_2004 defined LCA more generally as
virtually any statistical model where "some of the parameters [...]
differ across unobserved subgroups".

Despite the popularity of LCA,
two challenges impede broader adoption and correct application of the method.
The first challenge is that most existing software that implements these models is commercial and closed-source.
The second challenge is that there is a lack of standards for estimating and reporting LCA, and some myths and misunderstandings have been perpetuated in prior literature.
This introduces a risk of misapplications of the technique,
and complicates manuscript review and assessment of the quality of published studies.
The present paper seeks to address these challenges by introducing updated guidelines for estimatin and reporting LCA,
based on current best practice. 
Furthermore, meeting the increasing demand for open source research software,
this paper introduces new functionality in the R-package `tidySEM` [@vanlissaTidySEMTidyStructural2022] to perform LCA using free open source software (FOSS).

## The Argument for Free Open Source Software

Despite the popularity of latent class analysis (LCA),
most existing software is commercial and closed-source,
for example, Mplus [@muthenMplusUserGuide1998], for which the R-package `MplusAutomation` offers augmented LCA functionality [@hallquistMplusAutomationPackageFacilitating2018], and LatentGOLD [REF].
<!-- software that implements these models. -->
<!-- One important limitation is that the most fully-features -->
A downside of commercial software is that its use is restricted to individuals and institutions who can afford a license.
Another downside is that the proprietary source code cannot be audited, debugged, or enhanced by third parties.
This goes against open science principles [@lamprechtFAIRPrinciplesResearch2019],
incurs a risk that mistakes in the source code go unnoticed,
and curbs progress as independent developers cannot fix bugs or add functionality.

There are several compelling reasons why researchers should prefer free open source software (FOSS) instead.
FOSS is freely available for anyone to use, modify, and distribute, which means that scientists can access and use a wide range of powerful tools without financial impediments.
This is invaluable for reducing gatekeeping of researchers with limited budgets, such as those from developing countries.
Furthermore, FOSS promotes collaboration within the scientific community,
as its users can contribute to the development, maintenance, and support of the software.
Finally, FOSS promotes tranparency and reproducibility: Because the source code for FOSS is accessible,
researchers can review and verify the methods and algorithms used,
which helps to ensure the integrity and reliability of research findings.
Overall, FOSS has the potential to significantly improve the efficiency and effectiveness of scientific research, while also promoting transparency, collaboration, and inclusivity within the scientific community.

Notable FOSS for LCA includes Mclust [REF], depmixS4 [REF], and OpenMx [@neale_openmx_2016].
A crucial limitation of existing FOSS is that some packages have limited functionality (e.g., Mclust),
or implement specific innovations (e.g., depmixS4).
OpenMx is unique in that, like Mplus, it implement LCA in the context of a fully featured structural equation modeling framework.
Remaining obstacles to the use of `OpenMx` is that it is not very user-friendly and its functionality for LCA is poorly documented.

These limitations are addressed by new functionality in the `tidySEM` package,
which provide a user-friendly interface for LCA in `OpenMx`.
These functions adhere to best practices in estimation and reporting as described in this paper.
The `tidySEM` package aims to provide a tidy workflow for generating, estimating, reporting, and plotting structural equation models in a software-agnostic way.
The user interface is simple, as models can be specified using the widely used `lavaan` syntax [@rosseel_lavaan_2012].
The package contains user-friendly wrapper functions to conduct LCA with sensible defaults,
but the resulting models can still be fully customized (low barrier, high ceiling).
Specifically, LCA models constructed in `tidySEM` inherit from `OpenMx`' format `MxModel`, and remain compatible with downstream functions that process such models.
Being FOSS, `tidySEM` facilitates fully-reproducible analyses and contributes to open science.
It was developed with open science in mind,
and adheres to the FAIR software principles [Findable, Accessible, Interoperable, and Reusable, @lamprechtFAIRPrinciplesResearch2019],
and the [OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/en/criteria) for FOSS software.

## Defining Latent Class Analysis

Latent class analysis is a group of methods for estimating unobserved
groups based on a parametric model of observed indicators of group
membership [@vermuntLatentClassAnalysis2004].
This method has become quite popular across scientific fields,
under a number of different names;
most notably (finite Gaussian) mixture modeling and latent profile analysis.
The concept of LCA can be understood in different ways.
Generally speaking, LCA assumes that the study population is composed of $K$
subpopulations.
It further assumes that the observed data are
a mixture of data originating from those subpopulations;
hence the name mixture model.
Consider the simplest possible
univariate "model", which is a normal distribution.
This model has
two parameters: the mean and the variance.
LCA aims to estimate the values of those parameters across a known number of classes $K$,
as well as the probability of every individual $i$ belonging to classes $1 \ldots k$.
Commonly, the same model is estimated across all classes,
but with different parameters for each class (i.e., class-specific means and variances).

As an illustrative example, imagine that a detective wants to know if it
would be possible to use mixture modeling to identify the sex of a
suspect, based on footprints found at the crime scene. To test the
feasibility of this approach, the detective records the shoe sizes and
sex of 100 volunteers. The resulting observed data look like this:

```{r echo = FALSE}
library(tidySEM)
library(ggplot2)
set.seed(1)
n = 100
C <- sample(c("Man", "Woman"), n, replace = TRUE)
means <- c(Man = 9, Woman = 7)
X <- rnorm(n, mean = means[C], sd = 1)
X <- round(X*2)/2
obsparam <- tapply(X, C, mean)
shoesize_shapiro <- shapiro.test(X) # Shapiro-Wilk test of normality
```

```{r echo = FALSE, eval = run_everything}
est <- mx_profiles(data.frame(X), 2)
estparam <- table_results(est, c("label", "est", "matrix"))
estparam <- as.numeric(estparam$est[estparam$matrix == "M"])
props <- class_prob(est, "individual")$individual[ , "predicted"]
dput(props, "props.txt")
dput(estparam, "estparam.txt")

p <- ggplot(data.frame(Shoesize = X), aes(x = Shoesize)) + geom_density() + theme_bw()
ggsave("shoedens.png", p, device = "png")
```

```{r shoedens, echo = FALSE, fig.cap="Kernel density plot of shoe sizes."}
props <- eval(parse("props.txt"))
estparam <- eval(parse("estparam.txt"))
knitr::include_graphics("shoedens.png")
```

The distribution is evidently bimodal, which bodes well for the intended
mixture model. In this case, the number of classes is known a-priori.
When estimating a two-class mixture model, the detective observes that
the model estimates the mean shoe size of the two groups are equal to
`r report(estparam[1], equals = FALSE)` and
`r report(estparam[2], equals = FALSE)`, which is close to the true
means of the two groups, namely `r report(obsparam[1], equals = FALSE)`
and `r report(obsparam[2], equals = FALSE)`. When tabulating estimated
group membership against observed (known) group membership, it can be
seen that women are classified with a high degree of accuracy, but men
are not:

```{r tabshoe, results="asis"}
library(kableExtra)
tab <- table(C, props)
tab <- data.frame(Observed = rownames(tab), as.data.frame.array(tab))
names(tab)[2:3] <- c("Class 1", "Class 2")
rownames(tab) <- NULL
kbl(tab, caption = "Observed group membership by estimated class membership.")
```

Another way to conceptualize LCA is by analogy to a measurement model in structural equation modeling [@molenaarArbitraryNatureLatent1994].
A mixture model is like confirmatory factor analysis,
except with a categorical instead of a continuous latent variable.
One difference between
the two techniques is that factor analysis can be considered as a way to
group observed *variables* into latent constructs, and LCA groups
*individuals* into classes.
While factor analysis seeks to describe the relations between variables,
LCA seeks to describe unobserved groups and classify individual observations to those groups.
In line with this distinction, LCA
(mixture models) is referred to as a "person-centered" technique, and
factor analysis as a "variable-centered" technique
[@nylund-gibson_ten_2018; @masyn_latent_2013].

When the focus is on the model parameters in each group, LCA can be
thought of as similar to a multi-group structural equation model. The
main distinction is that group membership is not known a-priori, but is
instead estimated -- with measurement error -- based on the data.
Whereas in a multi-group model, the data are split by group and treated
as independent samples, in LCA, all cases contribute to the estimation
of all parameters in all groups. The relative contribution of each case
to the parameters of each group is determined by that case's posterior
probability of belonging to that group.

In factor analysis, the latent variable is continuous, so the relationships
between indicators are defined by the communalities between them. Here, the
latent variable represents the error corrected approximation of the
sample. In LCA, by contrast, the relation between indicators is defined by how
well they separate the classes, as a multinomial regression between the
indicators and the latent variable [@masyn_latent_2013].

When the focus is on each individual's estimated class membership,
LCA can be thought of as a type of clustering algorithm that corrects for (random) measurement error.
Specifically, the posterior class probability that an individual belongs to a latent class
can be computed from the likelihood of that individual's observed data under given the class-specific model.
These posterior class probabilities can be used to weight follow-up analyses,
or individuals can be assigned to classes based on their highest class probability.
If the classes are clearly distinct, measurement error is low, and the latter approach may be acceptable.
This perspective on mixture modeling is sometimes referred to as "model-based
clustering" [@hennig_handbook_2015; @scrucca_mclust_2016]. 
Many clustering algorithms apply some recursive splitting algorithm to the data. By
contrast "model-based" clustering refers to the fact that LCA estimates
cluster membership based on a parametric model.
Since a parametric model estimates only a fixed number of parameters,
it can be a relatively parsimonious solution compared to non-parametric techniques.

Finally, in the context of machine learning, LCA can be considered as an
*unsupervised classification* method [@figueiredo_unsupervised_2002].
The term *unsupervised* refers to the fact that the outcome variable --
true class membership -- is not known, and the term *classification*
refers to the fact that the algorithm is predicting a categorical
outcome -- class membership.

## A Taxonomy of Latent Class Analysis Methods

In this paper, we use the term latent class analysis to refer to a
family of techniques that estimate latent class membership based on a
parametric model of observed indicators. From a historical perspective,
the term was initially conceived to refer to analyses with categorical,
usually binary indicators [@vermunt_latent_2004]. Nowadays, there are
a number of related techniques, known by distinct names, that serve a
similar purpose. The term "latent class analysis" seems most appropriate
as an umbrella term for this broader class of models, as it only refers
to the purpose of the analysis, and does not imply restrictions to the
model used, or the level of measurement of the indicators. Given the
abundance of terms in use for closely related classes of models, we will
provide a rudimentary taxonomy of LCA methods.

A common type of LCA is the *finite Gaussian mixture model*; a
univariate analysis where the observed distribution of a single variable
is assumed to result from a mixture of a known number of normal
(Gaussian) distributions. The parameters of a finite Gaussian mixture
model are the means and variances of these underlying normal
distributions. The analysis of shoe sizes presented earlier is a
canonical example of this type of analysis. In the multivariate case,
with more than one indicator variable, the parameters of a mixture model
are the means, variances, and covariances between the indicators (which
can be standardized to obtain correlations). These parameters can be estimated 
freely, or set to be constrained across classes [@collins_latent_2009].

The technique known as *latent profile analysis (LPA)* is a special case
of the mixture model, which assumes conditional independence of the
indicators. Conditional independence means that, after class membership
is accounted for,
the residual covariances/correlations between indicators are
assumed to be zero.
In some cases, such constraints
will be inappropriate; for instance when the cohesion between indicators
is expected to differ between classes [@collins_latent_2009].
Consider, for example, a mixture model of ocean plastic particles, which found two classes of particles based on length and width:
A class of larger particles with a low correlation between length and width,
and a class of smaller particles with a high correlation between length and width.
The reason for this difference in correlations makes theoretical sense:
the large particles were heterogenous in shape,
whereas the smaller particles had been polished to a more uniform (rounded) shape by the elements [@alkema_maximizing_2022].

It is also possible to estimate a mixture model based on latent
indicators. This means that, within each class, one or more continuous
latent variables are estimated based on the observed indicators.
Categorical latent variable membership is then estimated based on these
continuous latent variables. A common application of this approach is in
longitudinal research, where the indicators reflect one construct
assessed at different time points. Examples of this approach include
*growth mixture models* (GMM) and *latent class growth analyses* (LCGA) 
[@jung_introduction_2008]. These techniques estimate a latent growth model 
to describe grouped trajectories over time. The growth mixture model is a 
latent class model where the parameters that indicate class membership are the 
intercepts and variances, and typically covariances of the latent growth 
variables, e.g., a latent intercept and slope. This technique assumes that
individuals within a class can have heterogenous trajectories. If the
variance of the growth parameters is fixed to zero, it is known as a
latent class growth analysis. This latter approach assumes that all
individuals within a class share the same identical trajectory, and that
any variance in the indicators not explained by the class-specific
latent trajectories is due to residual error variance.

The term latent class analysis originally referred to cases where the
observed indicators were ordinal (ordered categorical) [@collins_latent_2009]. 
Nowadays, it is more commonly used
as an umbrella term. To prevent ambiguity, the special case where
indicators are of binary or ordinal measurement level might be described
as *latent class analysis with ordinal indicators*.

<!-- Although this paper primarily focuses on latent class analysis with continuous indicators, -->
<!-- most of the suggested guidelines are equally applicable to ordinal indicators. -->

## Use Cases for Latent Class Analysis

There are several use cases for which LCA methods are suitable. 

### Testing theory

Although latent class analysis is often discussed as an exploratory analysis technique,
it can also be used in a confirmatory manner.
Given that latent class analysis is similar to confirmatory factor analysis,
but with a categorical latent variable, it can be used to similar ends:
To assess whether a theoretical measurement model holds.
This use case is relevant when testing a theory that postulates the existence of a categorical latent variable.
For example, *identity status theory* posits that, at any given point in time, adolescents reside in one of four identity statuses [@marcia_development_1966].
LCA can be used to identify these four statuses based on observed indicators (e.g., self-reported identity exploration and commitment) [@luyckxDevelopmentalTypologiesIdentity2008].
Similarly, *personality type theory* states that dimensional differences in personality can largely be explained by an undercontrolled, overcontrolled, and resilient type, which could also be restored using LCA [@donnellanResilientOvercontrolledUndercontrolled2010].
One challenge is that, unlike confirmatory factor analysis, LCA does not provide absolute fit indices.
Thus, to test a theory, one can ascertain that the data are better described by the number of classes dictated by theory than by a different number of classes.
Furthermore, it would be possible to test whether the observed pattern of class-specific means corresponds to a hypothesized pattern;
either qualitatively or quantitatively, using significance tests for pre-specified values.
A recent study by @maene_perceived_2022 applied this principle to 
test whether identities of Belgian adolescents with migration background can be 
summarized as those relating to their heritage, national and regional identity.

### Measurement model

Another use case relies on LCA's similarity to confirmatory factor analysis.
This approach is useful when class membership is measured imperfectly by several indicators.
This approach can be used to partial out measurement error and restore most likely class membership.
For example, a recent study found descrepancies between self-reported employment status and official register data. 
While it might be tempting to assume that the
register data is free of error, this has been shown to be false.
The authors used a mixture model to estimate the
"error corrected employment status" by using two indicators of the same question
and identifying random and systematic measurement error in both indicators 
[@pavlopoulos_measuring_2015].

This example also illustrates that LCA allows us to evaluate the reliability of different indicators [@geiser_is_2014].
High quality indicators will be strongly related to the
latent variable and will lead to good class separation.
A continuous indicator that discriminates well between classes will show large differences in class-specific means;
an ordinal indicator will show very high or very low conditional response probabilities for some classes but not for others.
This information can be used to select the indicators that are most diagnostic of class membership.
For an example of research on differential item functioning,
consider a study on university admission tests of Saudi Arabian students [@tsaousis_measurement_2020].
LCA revealed three
latent classes: high-, average-, and low-scorers. Subsequently, the authors
used the Multiple Indicator Multiple Causes model to identify both uniform
and non-uniform differential item functioning. The main finding was that gender
was a potential source of differential item functioning for latent class
indicators. Therefore, gender should be included as a covariate when fitting 
a LCA model in order to obtain unbiased estimates.

### Dimension reduction

LCA can also be used to reduce the dimensionality of data,
by reducing many variables to a few prototypes.
This is similar to the use of factor analysis,
with the key distinction that factor analysis accounts for linear covariance between indicators,
whereas LCA can accommodate complex - but discrete - patterns.
As this is a pragmatic application of LCA,
the existence of a true categorical latent variable is questionable (cf. the section on Testing theory).
Consequently, the results of this use case should be treated as descriptive and not "reified": i.e., the found classes should not be treated as evidence of the existence of a categorical latent variable.
The use of LCA for dimension reduction is common in longitudinal research,
where one developmental process is summarized using latent class growth analysis (LCGA),
and the resulting categories are then used as a moderator of another developmental process in a multi-group model.
For example, one study conducted a LCGA of adolescent empathy development on two dimensions of empathy and found three groups: high, average, and low empathy [@van_lissa_divergence_2015].
These groups were then used as moderator of a latent growth analysis of adolescent- and parent-reported conflict.
Results showed that the high-empathy group evidenced greater adolescent-parent agreement about the incidence of conflict than the other groups,
and that the low empathy group had more conflict with parents than the other groups.
Note that both these key findings are non-linear;
modeling them in a single-group model would be difficult.
LCGA greatly simplifies model specification and interpretation,
albeit at a cost of some loss of information and, potentially, generalizability.
<!-- Recently, @alkema_maximizing_2022 studied plastic debris in the Atlantic Ocean. -->
<!-- Prior to this study, ways to categorize ocean microplastics in terms of length, -->
<!-- width and polymer type were rather arbitrary, and assumed that these properties -->
<!-- were independent. Also, there was a lack of information regarding variability  -->
<!-- within these categories. The authors used finite Gaussian mixture modeling to  -->
<!-- build a classification system for ocean microplastic particles based on a set  -->
<!-- of measured characteristics. With regards to particle dimensions, two classes -->
<!-- emerged: one with smaller fragments whose width and height were highly  -->
<!-- correlated, and another consisting of larger fragments with low correlation  -->
<!-- between particle width and height. -->

When using ordinal indicators,
the need for dimension reduction can be especially pertinent.
For example, one study measured
fifteen dichotomous health indicators among injured military personnel [@macgregor_symptom_2021]. 
Combinatorics informs us that fifteen
dichotomous items have $2^{15}$ or $32,768$ unique symptom combinations.
However, LCA was able to reduce response patterns to five clinically meaningful latent classes.

### Class enumeration

Another use case is to explore potential heterogeneity in a population,
and to determine whether the population is comprised of latent subpopulations, and if so, how many.
The primary focus in this use case is on class enumeration.
For example, researchers suspected the existence of different risk profiles of substance use, sexual behavior, and mental health outcomes, and used LCA to determine how many such risk profiles could be distinguished [@hopfer_social_2014].
Another explored heterogeneity in the population of military service members and veterans with prior suicide attempts,
and identified two latent profiles characterized by high versus low suicide risk based on self-reported suicidal ideation and information about prior attempted methods and severity
[@gromatsky_characteristics_2022].

### Classification

Another use case is to estimate individuals' unobserved class membership based on observed indicators.
The shoe size example in Figure \@ref(fig:shoedens)
is a rudimentary illustration of this application.
In applied research, LCA is often used for classification in clinical contexts.
For example, LCA was used to determine clinical cut-off scores for diagnosing whooping cough [@baughman_mixture_2006].
Another study used LCA to not only classify the sample used to estimate the model, but also future cases [Zegwaard et al, in prep].
The study identified four types of care providers among those who supported close kin with mental health problems.
Importantly, the LCA model was embedded in an interactive app that could be used by healthcare professionals to determine the most likely class membership for new care providers, to provide more targeted support.
To obtain predicted class membership based on an existing LCA model,
the authors computed the likelihood of data from a new participant under the multivariate normal distributions estimated within all latent classes.
<!-- DA: this study is not in zotero yet, I lack information (title, authors, etc.) to enter it, @Caspar could you enter this to Zotero and adjust the in-text citation-->
These applications of LCA aid the clinical decision making process,
and help decide whether additional support is warranted,
or what type of intervention is appropriate.

### Violations of normality

Finally, LCA can be used to deal with data which violate certain
assumptions. As discussed in the shoe size example, LCA can deal with
violations of normality. In fact, LCA assumes the population
distribution is a non-normal mixture of $K$ normal distributions, and it
can recover the value of $K$, i.e. generate a $K$-class solution.
For instance, @alkema_maximizing_2022 studied ocean plastic debris. 
Plastic particle width and height are non-normally distributed. The authors 
demonstrate that the non-normal distributions of particle dimensions are
due to the measurements being a mixture composed of several particle types, 
each with its own normal distribution.

<!-- LCA and factor analysis share the approximation of an unobserved latent variable, the difference being if that variable is continuous or categorical. We encourage researchers to be agnostic about the **true** nature of the latent variable. And focus on the approach that fits the research question. -->
<!-- CJ: Why would we encourage researchers to be agnostic about the true...? -->

<!-- ### Controlling for covariates -->
<!-- CJ: I'm not sure we should call this a use case... in principle, -->
<!-- any time you're doing some kind of LCA you may want to predict group membership or predict something from group membership... -->
<!-- An extension of LCA is that containing covariates which can be used to -->
<!-- predict class membership. In this approach, we not only model the latent -->
<!-- class variable based on indicators, but we also relate the class -->
<!-- membership to other explanatory variables (@vermunt_latent_2017). An -->
<!-- example of using covariates comes from @nozadi_moderating_2016 who -->
<!-- applied LCA to identify the probability of children's membership to an -->
<!-- anxiety class. The authors tested several covariates including -->
<!-- children's age, sex, and accuracy scores. Age and sex were not found to -->
<!-- be related to the children's latent class membership, hence these -->
<!-- covariates were excluded from the analysis. Accuracy scores, however, -->
<!-- were related to probabilities of being in anxiety and attention-anxiety -->
<!-- classes. Therefore this covariate was kept as a valuable predictor. -->
<!-- Another example comes from @van_lissa_divergence_2015 who included sex  -->
<!-- as a predictor of the developmental trajectories and class membership in  -->
<!-- the previously discussed adolescent empathy development study. The rationale -->
<!-- was that adding sex as a covariate would account for known sex differences  -->
<!-- in empathy development, and lead to a model with lower bias. -->

<!-- ### Distal outcomes -->
<!-- CJ: I'm not sure we should call this a use case... in principle, -->
<!-- any time you're doing some kind of LCA you may want to predict group membership or predict something from group membership... -->
<!-- When our interest is the prediction of one or more outcomes, LCA can be used  -->
<!-- to construct latent classes as categorical predictors. @lanza_latent_2013  -->
<!-- demonstrated how LCA can be used to classify adolescents into depression  -->
<!-- classes, and subsequently these classes can be used to predict smoking, grades, -->
<!-- and delinquency. The study showed that the outcomes predicted by class  -->
<!-- membership can be binary (regular smoking), continuous (grades) or  -->
<!-- count (delinquency). In the previously discussed study of Belgian adolescent -->
<!-- identities, @maene_perceived_2022 first constructed the latent variable -->
<!-- representing different multiple identification strategies. Subsequently,  -->
<!-- the most likely class membership based on posterior class probability was -->
<!-- used as an "observed" categorical variable for prediction of depressive symptoms. -->

# Best Practices

The best practices in estimation, as outlined in Table
\@ref(tab:checkest), are rooted in existing recommendations for best
practices for estimating specific subtypes of LCA,
including latent
class growth analysis [@van_de_schoot_grolts-checklist_2017] and latent
class analysis with ordinal indicators [e.g., @nylund-gibson_ten_2018].
These were generalized to be relevant to all types of LCA, and updated
to current best practices, as explained below.
In order to aid researchers working with latent trajectory models,
@van_de_schoot_grolts-checklist_2017 developed a protocol called
Guidelines for Reporting on Latent Trajectory Studies (GRoLTS).

## Best Practices in Estimation

```{r checkest, include=TRUE}
tab_check <- data.frame(Item = c(
  "Examining Observed Data",
  "Handling Missing Data",
  "Alternative Model Specifications",
  "Software",
  "Algorithm",
  "Class Enumeration",
  "Model Fit Indices",
  "Classification Diagnostics",
  "Interpreting Class Solutions",
  "Label switching"
  
))
tab_check$`#` = paste0(1:length(tab_check$Item), ".")
kbl(tab_check[, c("#", "Item")])
```

### Examining Observed Data

Examining observed data is essential for any analysis as it may reveal
patterns and violations of assumptions that had not been considered
prior to data collection. Special attention should be paid to level of
measurement of the indicators.
Finite Gaussian mixture models (including
LPA) are only suitable for continuous variables.
Indicators with an
ordinal level of measurement are likely to violate the assumption of
within-class normal distributions of mixture models [see
@vermunt_k-means_2011].
Personal experience consulting on LCA methods
and moderating the `tidyLPA` Google group suggest that the
misapplication of mixture models to ordinal (e.g., Likert-type)
indicators is the most common source of user error. Whereas it has been
argued that some parametric methods are robust when scales with 7+
response categories are treated as continuous [e.g., @norman_likert_2010],
it is very unlikely
that an ordinal variable can be modeled as a *mixture* of multiple
normal distributions.
To illustrate the problem, consider what happens when estimating a 5-class solution on a 5-point Likert scale.
Each class-specific mean could describe a single response
category,
and a class-specific variance component would be nonsensical.
In sum, Likert-type scales are rarely suitable for Gaussian mixture modeling;
latent class analysis with ordinal indicators is more appropriate.

Extensive descriptive statistics (including the number of unique values,
variance of categorical variables, and missingness; see next subsection)
can be obtained using `tidySEM::descriptives(data)`. Note,
however, that sample-level descriptive statistics are of limited value
when the goal of a study is to identify subsamples using latent class
analysis. Plots (density plots for continuous variables, and bar charts
for categorical ones) may be more diagnostic. Note that density plots
can also aid in the choice of the number of classes, as further
explained in the section on visualization. Descriptive statistics and
plots can be relegated to online supplements, provided that these are
readily accessible [consider using a GitHub repository as a
comprehensive public research archive, as explained in
@van_lissa_worcs_2021].

### Data preprocessing
 
LCA can accommodate variables of different measurement levels: continuous, binary, nominal, and ordinal.
Numeric variables should have an interval measurement level,
such that it is sensible to estimate means and (co)variances.
Note that this assumption is likely violated for Likert-type scales.
For the remaining three measurement levels,
some data preprocessing is required.
The exact data types accepted by `tidySEM` at the time of writing are `numeric` and `ordered`.
To correctly specify ordered variables,
such as Likert scales
use the function `OpenMx::mxFactor()`.
Binary variables,
such as presence or absence of some symptom, are also treated as ordinal.
Finally, nominal variables are converted to binary indicators of group membership via dummy coding, for example, using `mx_dummies()`.

A second consideration in data preprocessing is that models with indicators on different scales may present with convergence issues.
The reason for this is that the parameter space is high-dimensional with many potential local optima.
If one indicator is on a much larger scale than another, the parameter space may be more extensively explored for that parameter.
The problem can be resolved by transforming the indicators to place them on (roughly) similar scales.
The most common transformation is standardization, which involves subtracting the mean of each variable from each value and dividing by the standard deviation.
In R, this transformation is applied using the `scale()` function.
Standardization retains all available information,
and model parameters can be transformed back to the original scale of the indicators by reversing the transformation.
The transformed variable has a mean of zero and a standard deviation of one.
Note that standardization is typically not necessary if indicators are already on the same, or similar, scales.

<!-- A third consideration in preprocessing relates to the univariate distributions of indicators. -->
<!-- For example, the `plas_depression` data included with `tidySEM` show extreme right skew; -->
<!-- these data assess depression, and as such,  -->


### Handling Missing Data

Previous work has emphasized the importance of examining the pattern of
missing data and reporting how missingness was handled
[@van_de_schoot_grolts-checklist_2017].
While we concur that investigating missingness is due diligence,
it is important to emphasize that missingness is adequately handled by default in `OpenMx`, which is the computational backend of `tidySEM`.
Its Full Information Maximum Likelihood (FIML) estimator makes use of all available
information without imputing missing values [@endersRelativePerformanceFull2001].
FIML is a best-practice solution for handling missing data;
at least on par with multiple imputation [@lee_comparison_2021].
Multiple imputation is less suitable to LCA for two reasons. First,
because LCA methods are often computationally expensive,
and conducting them on multiple imputed datasets may be unfeasible.
Second, because many multiple imputation methods have some assumption of normality, which is likely to be violated when it is assumed that the data come from a heterogenous population.
FIML is not suitable for cases with complete missing data.

Three types of missingness have
been distinguished in the literature [@rubin_inference_1976; 
@enders_applied_2022]: random missingness (MCAR);
missingness contingent on *observed* variables that can thus be known and controlled for (MAR);
and missingness related to unobserved variables, which can neither be known nor controlled for (MNAR).
A so-called "MCAR" test is often reported in relation to missingness
[@jamshidian_tests_2010].
But note that this name is somewhat misleading,
as a significant test indicates that missingness is MAR,
but a non-significant test statistic does not distinguish between MCAR or MNAR.
As the classic MCAR test relies on the comparison of variances across groups with different patterns of missing data, it assumes normality [@little_test_1988].
This assumption is tenuous in the context of LCA.
A non-parametric MCAR test may be more suitable [@jamshidian_tests_2010].
For this tutorial,
the lead author has implemented this test in the `mice` package as `mice::mcar()`.

To conclude; our recommendation is to inspect and report the proportion of missingness per variable (e.g., using `tidySEM::descriptives()`),
and to test for MAR using `mice::mcar()`.
As FIML estimation assumes that missingness is either MCAR or MAR,
one would proceed with FIML regardless of the outcome of the test.
One minor concern is that the approach `tidySEM` uses to determine starting values is *not* robust to
partially missing data.
This should not be a problem, however, as uses a global optimizer to estimate LCA models,
which should be independent of starting values [@coranaMinimizingMultimodalFunctions1987].

### Model Specification

Any LCA model should be carefully specified based on theoretical and empirical considerations.
The parameters of LCA depend on the measurement level of the indicators.
LCA with continuous indicators can be parametrized in terms of means, variances, and covariances.
Latent class growth analyses have the same parameters,
but with respect to the latent growth variables.
LCA with ordinal indicators can be parametrized differently.
The approach implemented in `tidySEM` assumes that each ordinal variable reflects an underlying standard normal distribution.
The parameters are "thresholds" that correspond to quantiles of a standard
normal distribution (with $N(\mu = 0, \sigma = 1)$).
These thresholds
are estimated based on the proportion of individuals in each of the
response categories of the indicator variable. For example, a binary
indicator has a single threshold that distinguishes the two response
categories. If responses are distributed 50/50, then the corresponding
threshold would be $t_1 = 0.00$. If the responses are distributed 60/40,
then the resulting threshold would be $t_1 = 0.25$.
All of the aforementioned parameters can be freely estimated,
or constrained across classes, or fixed (e.g., to zero).

Prior literature emphasized the importance of considering alternative model specifications [@vandeschootGRoLTSChecklistGuidelinesReporting2017].
However, what has remained underemphasized is that alternative model specifications may be approached differently depending on whether an analysis is exploratory or confirmatory (see Use Cases).
Most prior literature has emphasized exploratory applications of LCA [e.g., @nylund_deciding_2007]
In this context, model specification typically consists of an exhaustive grid search along several model specifications and varying numbers of classes.
A "final" model is then selected based on a combination of fit indices,
significance tests,
and interpretability (see Class Enumeration).

Confirmatory LCA does not require such an extensive search over the model space.
If hypothesized models have been specified a priori,
and possibly even preregistered,
it makes sense to focus on these models, rather than proceed in a purely data-driven manner.
Nonetheless, an important reason to consider alternative model specifications is that LCA lacks objective fit indices.
The fit of a theoretical model can thus only be assessed by comparison to other candidates.
For this reason, the theoretical model should still be compared to a few others.
A one-class model is a sensible benchmark, to test whether LCA is even appropriate.
Other candidates for comparison include competing theoretical models and those with a few more or less classes than the hypothesized number.
If the theoretical model has much better fit than other models,
this provides evidence for the theory.
If differences in fit are small, however, this may not  do not give cause to reject a theoretical model.
The theoretical model may still be preferred, for example, to ensure comparability with prior work.
The fact that the theoretical model received little differential support should of course be discussed,
along with potential implications for future work.


<!-- This will ensure that the model accurately represents the data and provides reliable results. -->
<!-- Two factors play a role in LCA model specification: -->
<!-- the number of latent classes and the model estimated within each class. -->
<!-- Assuming the same model is estimated across classes, -->
<!-- the total number of parameters in LCA is equal to the number of parameters of the class-specific model times the number of classes, -->
<!-- plus a vector of class proportions with $K-1$ free elements. -->

<!-- In the general sense, the class-specific model can be any structural equation model. -->
<!-- Its parameters may include means, (co)variances, regression coefficients, factor loadings, and thresholds for ordinal variables. -->
<!-- All of these parameters can be freely estimated, -->
<!-- constrained to be equal across classes, -->
<!-- or fixed to a known value. -->
<!-- The specific types LCA introduced in the Taxonomy all have different parameters. -->
<!-- For example, LPA typically assumes class-specific means, -->
<!-- equal variances across classes, and zero covariances (i.e., indicators are independent after accounting for latent class membership), see `mx_profiles()`. -->
<!-- By contrast, LCA with ordinal indicators assumes free thresholds across classes, -->
<!-- where each threshold reflects a proportion of responses within a particular response category, see `mx_lca()`. -->
<!-- Like LPA, LCGA and GMM typically have free means across classes, -->
<!-- but with respect to the latent growth variables instead of the observed variables, see `mx_growth_mixture()`. -->
<!-- The distinction between LCGA and GMM is whether variances of the latent growth variables are constrained to zero, or freely estimated (either across or within classes). -->
<!-- either using `lavaan` model syntax or using `OpenMx` RAM specification, -->
<!-- and supplied to the general function `mx_mixture()`. -->
<!-- The resulting model may have -->





<!-- HIER -->
<!-- For latent profile analysis, the function -->
<!-- `tidySEM::mx_profiles(classes, variances, covariances)` largely -->
<!-- automates this process. The argument `classes` indicates which class -->
<!-- solutions should be estimated (e.g., 1 through 6). The argument -->
<!-- `variances` specifies whether variances should be `"equal"` or -->
<!-- `"varying"` across classes. The argument `covariances` specifies whether -->
<!-- covariances should be constrained to `"zero"`, `"equal"` or `"varying"` -->
<!-- across classes. The means are free to vary across classes by default, -->
<!-- although the more general function `tidySEM::mx_mixture()` could be used -->
<!-- to circumvent this. After all models have been estimated, the function -->
<!-- `tidySEM::table_fit()` can be used to obtain a model fit table suitable -->
<!-- for determining the optimal model according to best practices. Note, -->
<!-- however, that this table does not include the bootstrapped likelihood -->
<!-- ratio test (BLRT) by default, because this test is very computationally -->
<!-- expensive. It is recommended to use the function `tidySEM::BLRT()` to -->
<!-- compare a shortlist of likely candidate models based on other fit -->
<!-- indices. More on fit indices can be found in the Model Fit Indices -->
<!-- subsection of this paper. -->

A second consideration in model specification is the risk of overfitting [@hastieElementsStatisticalLearning2009].
Overfitting means that a model captures idiosyncratic noise in a sample,
and thus generalizes poorly to new samples.
It occurs when a model has many parameters relative to the number of observations
As explained before, the number of parameters in LCA scales with the number of estimated classes.
Consequently, LCA with many classes have a potentially very high number of parameters.



In Bayesian mixture modeling,
it was observed that if the estimated number of classes exceeded the true number, the number of observations assigned to these classes tended towards zero as sample size increased [@rousseauAsymptoticBehaviourPosterior2011].
Although it is not known whether frequentist LCA exhibits the same behavior,
this suggests that low class counts might indicate overfitting.


```{r, include = FALSE}
grolts <- structure(list(Number = c("1.", "2.", "3a.", "3b.", "3c.", "4.", 
"5.", "6a.", "6b.", "7.", "8.", "9.", "10.", "11.", "12.", "13.", 
"14a.", "14b.", "14c.", "15.", "16."), Item = c("Is the metric of time used in the statistical model reported?", 
"Is information presented about the mean and variance of time within a wave?", 
"Is the missing data mechanism reported?", "Is a description provided of what variables are related to attrition/missing data?", 
"Is a description provided of how missing data in the analyses were dealt with?", 
"Is information about the distribution of the observed variables included?", 
"Is the software mentioned?", "Are alternative specifications of within-class heterogeneity considered (e.g., LGCA vs. LGMM) and clearly documented? If not, was sufficient justification provided as to eliminate certain specifications from consideration?", 
"Are alternative specifications of the between-class differences in varianceâ€“covariance matrix structure considered and clearly documented? If not, was sufficient justification provided as to eliminate certain specifications from consideration?", 
"Are alternative shape/functional forms of the trajectories described?", 
"If covariates have been used, can analyses still be replicated?", 
"Is information reported about the number of random start values and final iterations included?", 
"Are the model comparison (and selection) tools described from a statistical perspective?", 
"Are the total number of fitted models reported, including a one-class solution?", 
"Are the number of cases per class reported for each model (absolute sample size, or proportion)?", 
"If classification of cases in a trajectory is the goal, is entropy reported?", 
"Is a plot included with the estimated mean trajectories of the final solution?", 
"Are plots included with the estimated mean trajectories for each model?", 
"Is a plot included of the combination of estimated means of the final model and the observed individual trajectories split out for each latent class?", 
"Are characteristics of the final class solution numerically described (i.e., means, SD/SE, n, CI, etc.)?", 
"Are the syntax files available (either in the appendix, supplementary materials, or from the authors)?"
), relevant = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, 
TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, 
TRUE, TRUE, TRUE)), row.names = c(NA, -21L), class = "data.frame")
```


### Algorithm

LCA parameters and model fit statistics can be estimated in a variety of
ways. The choice of the estimator depends on the presence of missing
values, sample size, number of indicators, and available computational
resources [@weller_latent_2020]. A commonly used technique is maximum
likelihood (ML) estimation with the expectation-maximization (EM)
algorithm as a local optimizer. Imagine we are estimating two
parameters, e.g. the class-specific means $\mu_c$ on a continuous
(ignoring the variance for now).
EM attempts to find a combination of values for these two parameters that maximizes the log likelihood ($LL$) of the data.
In practice, instead of maximizing $LL$,
$-2*LL$ is minimized.
Transforming the likelihood this way does not change the resulting parameter estimates,
but it is mathematically convenient.
Moreover, the $-2*LL$ is asymptotically $\chi^2$ distributed if the null-hypothesis is true, and thus this quantity can be used for model fit tests [@wilksLargeSampleDistributionLikelihood1938].
To understand the optimization problem, imagine a three-dimensional landscape: 
The X and Y dimensions represent potential values of the class-specific means, $X = \mu_1$ and $Y = \mu_2$, and the Z-dimension is determined by $Z = -2*LL$. 
The optimizer must find the
deepest "valley" in this landscape,
which reflects the combination of $\mu_1$ and $\mu_2$ that minimizes the $-2*LL$.
The EM
optimizer behaves somewhat like a marble dropped in this landscape. It
is dropped at some random point in space, and will roll into the nearest
valley.
The challenge is that LCA models are often highly parametrized.
This means that the landscape is not three-dimensional, but (very) high-dimensional,
with potentially many valleys and sparse data to provide information about which valley is deepest.
If the marble rolls into one of these valleys,
it will settle there and not climb out again.
The estimator has "converged".
The risk is that the optimizer gets stuck in a shallower valley (a "local optimum"),
and never discovers the deepest valley (the "global optimum", or best solution).
One solution to this problem is to drop many marbles at random places,
compare their final $-2*LL$ values,
choose the solution
with the lowest $-2*LL$, and make sure that several marbles replicated
this solution.
This is the "random starts" approach implemented in, e.g., Mplus.

One problem with the random starts approach is that it is
computationally expensive to run this many replications. Moreover,
because the algorithm begins with random starting values,
it is inefficient because many marbles are likely to start very far away from any valley.
Finally, if all sets of starting values are close to a local optimum,
there is a risk that the global optimum is not discovered.
An alternative solution that overcomes these challenges is to use a global optimizer, like the simulated annealing algorithm [SA, @coranaMinimizingMultimodalFunctions1987].
SA does not behave quite like the marbles of the random starts approach;
instead, it iteratively considers some "destination" in the landscape,
and compares its $-2*LL$ to the current one.
If the destination $-2*LL$ is lower, 
the estimator moves there.
However, if the destination  $-2*LL$ is *higher*,
the estimator still moves there occasionally, with a certain probability.
Think of this as occasionally flicking the marble back out of valleys,
to see if it rolls back into the current valley, or finds another,
even deeper one.
This property allows SA it to escape local optima and find the global optimum.

Since SA is a global optimizer, it should be independent of starting values.
With this in mind, it is efficient to start estimation from a "reasonable solution", instead of a random starting point.
For example, if we assume that the different classes are likely to have different mean values on the indicators, 
then a nonparametric clustering algorithm - like K-means or hierarchical clustering - can be used to determine these cluster centroids [see also @biernackiChoosingStartingValues2003].
By default, `tidySEM` derives starting values using K-means clustering,
computing the mixture model starting values by treating the K-means solution as multi-group model.
Next, SA is used to find the global optimum solution.
Finally, SA is followed up with a short run of the ML algorithm,
as EM inherently produces an asymptotic
covariance matrix for the parameters that can be used to compute
standard errors.
Note that these defaults can be manually overridden.

In case of any (suspected) convergence problems,
note that `tidySEM` LCA models are `OpenMx` models.
Thus, existing `OpenMx` functions to aid model convergence can be used, like `mxTryHard()` or `mxTryHardWideSearch()` for continuous indicators, and `mxTryHardOrdinal()` for ordinal indicators.
Often, these functions return an acceptable solution for models with convergence issues.

### Class Enumeration

<!-- The total number of parameters scales with the number of estimated classes. -->
<!-- Consequently, latent class analyses have a potentially very high number of parameters. -->
<!-- As any of these parameters could be mis-specified, -->
<!-- it is important to consider alternative model specifications. -->
<!-- One way to obtain relative evidence in favor of different models is by using BIC weights, obtained by the function `tidySEM::ic_weights()` [@wagenmakersAICModelSelection2004]. -->

<!-- From sinhaPractitionerGuideLatent2021: -->
<!-- Testing for number of classes. In addition to indexing model fit, there are tests comparing a model with k classes to one with k-1 classes. Developed by Lo, Mendell and Rubin (36) based on work by Vuong (54) the VLMR test assumes multivariate normality, and it is not clear how sensitive the p-value is to violation of that assumption. It is also possible to use a bootstrapped p-value, although its validity outside of normally distributed data remains unknown.(55) In a Monte Carlo simulation of LCA using only categorical indicators, Nylund and colleagues found that the bootstrapped test (BLMR) consistently outperformed the simple Lo-Mendell-Rubin test (LMR).(34) Again in simulation studies with latent profile analysis (continuous indicators), Tein and colleagues found that the BLMR had higher statistical power than LMR, although both tests performed well at detecting the true class.(56) In our practice, across multiple analyses with real-life data consisting of a mixture of categorical and continuous indicators, we have found that the BLMR consistently favours k classes over k - 1 class to the point of being of limited value. -->
Class enumeration refers to the process of determining the appropriate number of classes.
This topic is closely related to model specification,
because the number of latent classes also influences the number of model parameters.
As explained before,
LCA can be done in an exploratory or in a confirmatory fashion.
In the exploratory case, the set of models typically consists of a sequence from 1 to some higher number of classes $K$.
The maximum number of classes $K$ may be chosen a-priori, or on theoretical grounds, or dictated by the data:
after a certain number of classes,
convergence problems often arise, 
or the proportion of cases assigned to the smallest classes may be so low that such a class would not be locally identified.
In Bayesian LCA, it has been observed that, when the estimated number of classes exceeds the true number,
the proportion of cases assigned to the excessive classes converges to zero as the sample size increases [@rousseauAsymptoticBehaviourPosterior2011].
It is not known whether this phenomenon also occurs in the frequentist case;
nonetheless, near-zero class memberships should be taken into account when determining the maximum $K$.
It is sensible to limit $K$ to the maximum number of classes that result in valid and interpretable solutions.
In the confirmatory case,
the set of models with different numbers of classes to be compared is typically smaller;
its purpose is to provide context for the relative fit of the theoretical model.

During class enumeration, the set of models is compared on several statistical and substantive criteria.
In the exploratory case, these criteria may be decisive in selecting a final model;
in the confirmatory case, the burden of proof is reversed: there must be convincing evidence against the theoretical model in order to reject it.
Statistical criteria to consider in class enumeration include model fit indices and
likelihood ratio tests [@weller_latent_2020; @masyn_latent_2013].
Substantive criteria include convergence and model identification,
class separability,
and interpretability.
We address each of these criteria below.

<!-- In exploratory LCA, a sequence of models is fitted to the data -->
<!-- with each additional model estimating one more class than the previous -->
<!-- model. These models are then compared and the best solution is selected -->
<!-- as the final class solution. In some cases, prior theory can inform the -->
<!-- researcher about the number of classes to expect. Even in such -->
<!-- confirmatory LCA cases, it is nonetheless useful to know if the -->
<!-- theoretical model is markedly better than those with differing numbers -->
<!-- of classes. Therefore, it may always be useful to compare different -->
<!-- class solutions. -->

### Model Fit Indices

As there are no absolute fit indices for LCA,
information criteria are typically used to assess relative fit.
Information criteria are computed from the -2\*log-likelihood ($-2LL$),
which is the most fundamental measure of model fit.
A penalty for model complexity is added to the $-2LL$ to penalize more complex models.
Information criteria thus balance fit and complexity.
The lower
the value of an information criterion, the better the overall fit of the
model.
The original information criterion is the Akaike Information Criterion (AIC),
computed as $-2LL+2p$, where $p$ is the number of parameters.
The Bayesian Information Criterion (BIC) multiplies the penalty for complexity with a function of the sample size, $p \ln(n)$,
such that complex models are penalized more in smaller samples.
Finally,
the so-called *sample size adjusted* BIC (saBIC) implements a different ad-hoc penalty for sample size,
$\ln(\frac{n + 2}{24})$.
Although a simulation study showed that the saBIC performed well for class enumeration,
its penalty is not based on theory.
BIC also performs well and is theoretically substantiated;
as such, it may be the most appropriate information criterion to use for model comparison [@nylund-gibson_ten_2018; @masyn_latent_2013]. 
All three ICs are available in the output of `table_fit()`.

Information criteria may occasionally contradict each other, so it is
important to identify a suitable strategy to reconcile them.
One option is to select a specific fit index, such as the BIC,
before analyzing the data.
Another option is to always prefer the most parsimonious model that has best fit according to any of the available fit indices.
Yet another option is to incorporate information from multiple fit indices using the analytic hierarchy process [@akogul_comparison_2016],
which is implemented in `tidyLPA` [@rosenbergTidyLPAPackageEasily2018].
Finally, one can use a scree plot to determine the inflection point at which additional classes contribute little to further decreases of the ICs [@nylund-gibson_ten_2018].
The preferred number of classes is at the inflection point where the slope of the curve is clearly leveling off.
Adding this many classes results in substantial decreases of the IC,
and adding further classes results in relatively small decreases.

Given a specific choice of information criterion,
it is also possible to calculate so-called "IC-weights",
which indicate the relative support the data provide for each model in the set, expressed as a proportion of total support [@wagenmakersAICModelSelection2004].
In `tidySEM`, IC-weights can be calculated by calling `ic_weights(x = table_fit(), ic = "BIC")` on a model fit table.
Note, however, that IC-weights will (strongly) prefer the model with the lowest IC value;
the scree plot approach above may be more suitable to select the most parsimonious model with adequate fit.

<!-- The general objective of information criteria is to evaluate the model's  -->
<!-- out-of-sample predictive accuracy, -->
<!-- thus curtailing overfitting. -->
<!-- Fit measures such as $R^2$ evaluate the in-sample predictive accuracy,  -->
<!-- i.e. the model's ability to predict the observed outcomes based on the same  -->
<!-- data that was used to fit the model. In-sample metrics are positively biased,  -->
<!-- meaning that they overestimate the fit the model has in reality, i.e. when  -->
<!-- presented with new data. While common ways to overcome this bias is through -->
<!-- resampling methods such as cross-validation, or use of out-of-sample data such -->
<!-- as a test set, ICs correct for this positive bias by evaluating the model's  -->
<!-- accuracy in a way which approximates the out-of-sample predictive accuracy.  -->
<!-- ICs achieve this by applying different penalty metrics. For example, $R^2$ will  -->
<!-- increase even if an added predictor is fully redundant. ICs show preference for -->
<!-- simpler models and indicate worse fit when a predictor, or its added complexity  -->
<!-- is unnecessary [@mcelreath_statistical_2020].  -->

### Nested model tests

When two models are nested, the significance of the difference in fit between these models can be tested using a Likelihood Ratio test (LR).
The test statistic is computed as the -2 times the log of the ratio of likelihoods,
and its degrees of freedom are equal to the difference in the degrees of freedom of the two compared models.
Since a $k-1$-class model is nested in a $k$-class model,
it is possible to test the difference in model fit using a LR test.
One challenge is that the LR test statistic is not asymptotically $\chi^2$ distributed in LCA.
The so-called Lo-Mendell-Rubin adjusted LR test (LMR) overcomes this problem by applying an ad-hoc correction to the LR [REF lo 2001].
This test is implemented in `tidySEM`, and is part of the default output of `table_fit()`.

Another solution is to derive an empirical sampling distribution for the LR difference.
The so-called bootstrapped Likelihood Ratio Test (BLRT) does so by simulating data from the $k-1$-class model $B$ times (e.g., 1000),
and fitting the $k$- and $k-1$-class models to each simulated dataset [REF Nylund et al., 2007].
The p-value of this test is defined as the proportion of $B$ in which the LR of the models fit to simulated data exceeds the LR observed in the real data.
A crucial limitation of the BLRT is that it is very computationally expensive.
With this in mind, `table_fit()` does not provide the BLRT by default.
It is possible to compute it for a shortlist of likely candidate models,
using the function `tidySEM::BLRT()`.
However, we have noticed that this function often runs out of memory or takes prohbitively long to compute.

Simulation studies comparing the LMR and BLMR suggest that the latter has greater statistical,
although both performed comparably well at detecting the true number of classes [REF Nylund 2007, REF Tein 2013].
However, anecdotal experience as a statistical co-author on LCA studies suggests that both these tests tend to be liberal,
returning significant p-values for models with more classes even when other indicators suggest that those models are overfitted (e.g., near-zero class counts).
Other authors have reported similar experiences [@sinhaPractitionerGuideLatent2021].
This suggests that, like existing ICs,
LR tests do not always adequately balance fit and complexity in LCA.
Better solutions for class enumeration are thus an important target for future research.

One promising alternative solution is the so-called *lazy bootstrap* (LB),
which is much more efficient because it is based on sample statistics of model-implied data generated from the $k-1$ and $k$-class models,
rather than on fit statistics of LCA models fit to those data [@van_kollenburg_lazy_2018].
It is similar to the posterior predictive p-value in Bayesian statistics,
and describes how well each model is able to reproduce the observed data [see @mcelreath_statistical_2020].
One version of the lazy bootstrap is already implemented in the commercial program latentGOLD.
At the time of writing, we are developing extentions of the LB and investigating their utility for class enumeration.
Future versions of `tidySEM` will likely incorporate this test.

### Classification Diagnostics


<!-- 1. We do not consider classes with, on average, fewer than 5 participants per parameter in a class due to potential local underidentification -->
<!-- 1. We do not consider solutions with entropy < .90 because poor class separability compromises interpretability of the results -->
<!-- 1. We do not consider solutions with minimum posterior classification probability < .90 because poor class separability compromises interpretability of the results -->

<!-- Nevertheless, researchers can set an a-priori threshold for the minimum proportion of cases per class, -->
<!-- for example by considering only solutions with a number of participants at least five times as large as the number of parameters per class. -->

In most use cases of LCA, the goal is to identify 
subgroups that are internally homogeneous and externally distinct.
Classification diagnostics assess whether this is the case.
These diagnostics are distinct from model fit indices,
because a model can fit the data well but show poor latent class separation [@masyn_latent_2013].
Classification diagnostics should therefore not be used for model selection.
Nonetheless, they can help restrict the search space of acceptable models.
Interpretability should always be a consideration when considering
different class solutions [@nylund-gibson_ten_2018].
If the goal is to interpret properties of (members of) classes,
it makes sense to consider only models that meet certain minimum thresholds for classification diagnostics.

All classification diagnostics are derived from the posterior classification probabilities: an $n \times k$ matrix with the probability that every individual $n$ belongs to latent class $k$.
When classification accuracy is high, and classes are distinct,
each individual's posterior class probabilities are high for one class, and low for the remaining classes.
This matrix of individual classification probabilities $P$ is obtained by running `class_prob(res)` and extracting the element labeled `"individual"`.
Note that one column is added to the matrix $P$:
the final column represents most likely class membership $M$,
which is obtained by assigning each observation to the class corresponding to their maximum posterior class probability.
Thus, if an individual has posterior probabilities `c(0.2, 0.5, 0.3)` for classes 1-3,
they would be assigned to class 2 ($M = 2$).
The predicted class membership is often of interest to researchers who wish to carry out follow up analyses.
Note, however, that such follow-up analyses should take classification error into account. The function `BCH()` does this automatically.

The matrix $P$ can be summarized in different ways, and the following summaries are provided by default:
First, a frequency table of the latent categorical variable,
based on posterior probabilities, referred to as $F_P$ (`"sum.posterior"`).
It is obtained by calculating the column sums of $P$.
These numbers are fractional because individuals can contribute partially to multiple classes.
This table thus takes classification error into account.

Second, most likely class membership based on the highest posterior class probability, $F_M$ (`"sum.mostlikely"`).
This is the frequency table of the variable $M$, and it ignores classification error.
Note that this table can differ from $F_P$,
where some observations may partly contribute to multiple classes.
If every observation is classified with perfect accuracy,
these tables will be identical.
Part of the output of `table_fit()` for LCA analyses
are the minimum and maximum percentage of the sample assigned to a
particular class, `n_min` and `n_max`, which are derived from $F_M$.
Of particular interest is the smallest class proportion, `n_min`.
When the smallest class (`n_min`) is less than 10% of the total sample size,
proceed with caution:
The model might not be locally identified in that class.
Check whether the number of participants in this class is larger than the number of parameters per class (e.g., five times as large),
and check whether this smallest class has a meaningful substantive interpretation.

To assess the classification accuracy, we turn to the third diagnostic table:
The average posterior probabilities by most likely class membership, $P_{p_{(M = m)}}$ (`"avg.mostlikely"`):
for participants assigned to one particular class (rows) according to $M$,
this table gives the mean probability of being assigned to any class (columns).
This table is obtained by calculating the column means of $P$ for the 
$k$ subsets of observations with most likely class of $M = 1 \ldots k$.
The diagonal represents the probability that observations in each class will be correctly classified.

Finally, the classification probabilities for the most likely latent class $M$ by latent class, $P_{p_{(M = m | C = c)}}$ is similar to the aforementioned table $P_{p_{(M = m)}}$,
except that it accounts for classification uncertainty.
If $C$ is the true class of an observation, which is imperfectly measured,
and $M$ is the most likely class,
then this table shows the probability of an observation being assigned to class $m$ in $M$, given that its true class is $c$ in $C$, or $p(M=m|C=c)$.
Values on the diagonal can thus be conceptualized as the reliability with which each level of the categorical latent variable is measured by $M$,
and off-diagonal elements can be conceptualized as measurement error.
The minimum and maximum of the diagonal of this table are given by default as part of the output of `table_fit()` for LCA analyses (`prob_min` and `prob_max`).
These probabilities are a convenient way to summarize classification accuracy.
The minimum probability is especially important however,
as it can diagnose if there is at least one class with poor classification accuracy.

Finally, class separability can be summarized in a single statistic:
the entropy criterion.
In physics, the concept of entropy is associated with randomness or uncertainty.
High entropy corresponds to maximum uncertainty,
and low entropy corresponds to a strict arrangement of the units of study.
Applied to LCA, high entropy means that classes are completely indistinguishable; every individual has equal probability of being in any class.
Low entropy, by contrast, means that every individual has a near-one probability of being in one class only, and near-zero probability of being in any other class.
Crucially, in `tidySEM`, the entropy criterion is defined as 1-entropy (a convention originating in Mplus).
The interpretation of the entropy criterion is reversed:
0 means the model classification is no better than chance,
and 1 means perfect classification. 
<!-- Celeux, G., & Soromenho, G. (1996). An entropy criterion for assessing the number of clusters -->
<!-- in a mixture model. Journal of Classification, 13, 195-212. -->
As a rule of
thumb, values above .80 are deemed acceptable and above .90 are considered good.
Values close to 1 indicate that classes are very well separated,
and treating the most likely class membership $M$ as observed in follow-up analyses is probably defensible.
Like other classification diagnostics,
entropy is not a model fit measure and should not be used for model selection [@masyn_latent_2013].
However, it can be used to disqualify certain solutions if the goal is to identify clearly distinct classes.

### Interpreting Class Solutions

An important outcome of LCA are conditional item probabilities, also
known as class-specific item probabilities [@masyn_latent_2013],
conditional response, or conditional solution probabilities
[@geiser_data_2012]. They indicate the probability of an item being
endorsed given that the observation belongs to a particular latent
class. Conditional item probabilities can be obtained using
`tidySEM::table_prob()`. If a particular item is endorsed by two or more
classes at markedly different rates, it is said to discriminate well
between the classes and is consequently considered a good indicator.
Classes are considered highly homogeneous with respect to an item when
for a particular item there is a distinct difference in conditional item
probabilities for two or more classes. For instance, if an item is
endorsed below 30% for one class and above 70% for another class, the
classes have high homogeneity with respect to this item
[@masyn_latent_2013]. Conditional item probabilities are the analogue of
mean and standard deviation when the indicators are binary or ordinal.

A problem which can occur is that of inadmissible solutions. With binary
indicators, LCA is modeling a cross-table with all the predictors. The
problem with such cross-tables is that they will often contain empty
cells, i.e. combinations of responses that never occur together. This
problem is reflected by extreme conditional item probabilities (as in
exactly 0 or 1). Such boundary parameter estimates could indicate that
the solution is invalid [@geiser_data_2012]. Boundary parameter
estimates can also happen with continuous indicators. For instance, if
we have a zero-inflated normal distribution and a two class solution,
one class might have the mean of zero and its standard deviation cannot
be determined since there is little variance. This too could be a sign
of an invalid solution, warn that too many classes were extracted, or
indicate a local optimum [@geiser_data_2012].

### Label Switching

The final class solution will usually discover and enumerate several
classes. The class ordering, however, is completely arbitrary. The class
labeled as Class 1 in one solution may become Class 2 or Class 3 in
another model, even when the only difference between the models is in
their starting values. Label switching is something to be mindful of
when comparing different LCA models [@masyn_latent_2013].

The order of clusters is nondeterministic when using K-means in
`tidySEM`.
Therefore label switching is still a consideration.
A simple solution to this is setting a random seed number one line prior to
fitting the model, using `set.seed()`.
We advise `tidySEM` users to always do so in order to
circumvent label switching.
Another solution is to specify starting values manually,
but this is more challenging for two reasons:
First, because this requires modifying the underlying `MxModel` objects, which is not straightforward.
Second, if the starting values are not very good, label switching may still occur, and bad starting values could even lead to non-convergence.

Class names should be chosen to accurately reflect group membership,
as theoretically relevant names according to the interpretation of the class solutions.
Overly simplified and generalized class names may prove misleading 
to both audiences and researches alike leading to what is known as a naming
fallacy [@weller_latent_2020].

## Best Practices in Reporting

There is substantial merit in pre-specifying, and ideally preregistering, the criteria used for class enumeration.
This is especially pertinent for LCA because there are no objective fit indices for LCA,
and because there is a risk that information criteria and likelihood ratio tests will suggest a larger number of classes, even if there are concerns about overfitting, classification accuracy, or model interpretability.


Among studies using LCA, reporting practices vary significantly
[@weller_latent_2020].
Various authors have tried to improve and
standardize ways of reporting LCA [e.g. @masyn_latent_2013;
@weller_latent_2020], but more work is needed. 
<!-- This does not belong here: -->
<!-- @van_lissa_worcs_2020 -->
<!-- developed WORCS, a workflow for open reproducible code in science. WORCS -->
<!-- consists of step-by-step guidelines for research projects based on the -->
<!-- TOP-guidelines developed by @nosek_promoting_2015. WORCS workflow can be -->
<!-- easily implemented in R in form of an R package which facilitates -->
<!-- preregistration, article drafting, version control, citation and -->
<!-- formatting, among others [@van_lissa_worcs_2020]. -->

TOP-guidelines emphasize the use of comprehensive citation (including
referencing the software used in the analysis), as well as code and data
sharing wherever possible [@nosek_promoting_2015]. @van_lissa_worcs_2020
If the original data cannot be shared, sharing synthetic data is a good alternative.
The function `worcs::synthetic()` provides a generic, non-parametric method to create a synthetic copy of data.
When synthesizing data for LCA, it is also possible to synthesize data directly from the model.
LCA models constructed in `tidySEM` are estimated using `OpenMx`,
and as such, can benefit from all existing functions in that package.
To synthesize data from a mixture model `mix`, use the function `mxGenerateData(mix)`.

the entire research project is made reproducible so that others may
download it and reproduce it with just one click; for guidance, see
@van_lissa_worcs_2020.

As the open science movement is gaining momentum, researchers are
becoming increasingly aware how important it is that analyses can be
reproduced and audited. In line with open science principles, one of the
suggested reporting standards relates to reproducible code. In this
context, it is important to note that user-friendly methods for
estimating LCA models have predominantly been available in commercial
software packages (e.g., *Mplus* and *Latent GOLD*). A potential
downside of commercial software is that it restricts the ability to
reproduce analyses to license holders, and prevents auditing research
because the underlying source code is proprietary. To overcome these
limitations, the present paper introduces new user-friendly functions in
the `tidySEM` R-package that can be used to estimate a wide range of LCA
models using the free, open-source R-package `OpenMx` as the backend. The 
reporting guidelines described in this paper are adopted in `tidySEM` by 
default. The `tidySEM` R-package thus makes advanced mixture modeling based 
on best practices widely accessible, and facilitates the adoption of the
estimation and reporting guidelines described in this paper.

### Visualization

Plots can greatly improve the interpretability of LCA models. There are
several stages where this is an important consideration.

First, in class enumeration, we want to compare several competing class
solutions. This can be done by means of the AIC and BIC. Here, we
suggest using an elbow plot with the $K-class\ solution$ on the X-axis,
and information criteria value on the Y-axis. For continuous variables, 
density plots can aid class enumeration. We demonstrate how to implement 
both elbow and density plots in the Tutorial subsection of this paper.

Second, once we have decided on the final class solution, we want to
interpret the response patterns on the indicators for each latent class.
While `tidySEM` can create a table showing the probability of each
item's response endorsement (using `tidySEM::table_prob`), it may be
easier to inspect these probabilities visually. For this reason, we
created `ggplot2::plot_prob()`. The resulting shows response patterns on
all indicators for each group. We give an example of this plot in the
Tutorial subsection.

## Myths and misunderstandings

Relatedly, a recent publication claimed that an assumption of mixture
models is that observed indicators are normally distributed
[@spurk_latent_2020]. This is incorrect. When the number of classes is
greater than one, Gaussian mixture models assume that the observed indicators 
are a mixture of multiple (multivariate) normal distributions. In our shoe
size example, it can be seen that the population distribution is
comprised of two normal distributions. When examined visually, the
population distribution is evidently bimodal. The Shapiro-Wilk normality
test ($W `r report(shoesize_shapiro[["statistic"]])`$, $p `r report(shoesize_shapiro[["p.value"]])`$ ) rejects the null hypothesis that the
sample comes from a normally distributed population. Yet, this is a
prototypical example of a mixed population distribution where LCA can
discover latent groups. If the population distribution were instead
normal, there would be no classes to extract as the whole population
would belong to a single (homogeneous) class.

A recent paper suggested maximum likelihood with robust standard errors
should be used when the observed indicators are not normally distributed
[@spurk_latent_2020].
This statement is incorrect;
LCA does not have an assumption of marginal normality, and there is - to our knowledge - no literature on the merits of robust standard errors for LCA.

<!-- and may lead readers -->
<!-- to believe that they must use commercial software, as robust maximum -->
<!-- likelihood is currently only implemented in Mplus and LatentGOLD. As -->
<!-- explained before, mixture modeling assumes that observed data are a -->
<!-- mixture of (multivariate) normal distributions; thus, the observed -->
<!-- indicators will likely not be normally distributed. -->
<!-- CJ: there is no literature that says robust standard errors are better with latent class analysis? -->

<!-- Treating ordinal data as continuous -->
<!-- People omitting the 1-class solution in class enumeration -->


# Tutorial

This is an example of exploratory LCA with ordinal indicators using `tidySEM`.
At the time of writing, the `tidySEM` package includes two more vignettes to demonstrate latent profile analysis (LCA with continuous indicators),
and Latent Class Growth Analysis.
The present example uses a simulated dataset with ordinal indicators included in `tidySEM`.
To view its documentation,
run the command `?tidySEM::data_mix_ordinal` in the R console.

## Loading the Data

To load the data, one simply needs to attach the `tidySEM` package.
For convenience, we assign the data to an object called `df`:

```{r, include = TRUE, eval=F}
# Load required packages
library(tidySEM) 
# Load data
df <- data_mix_ordinal
```

## Examining the Data

As per the best practices,
the first step in LCA is examining the observed data.
We use `tidySEM::descriptives()` to describe the data.

```{r, include = TRUE, eval=F}
tidySEM::descriptives(df)
```

As we can see, the output includes various descriptives of our dataset.
For example, it provides invaluable information about the measurement level of the indicators,
which is used to specify the model correctly.
If these data had not been coded as ordinal factors,
these descriptive statistics would have revealed that each variable has only 3 unique values.
Attention should alo be paid to the pattern of
missingness, as discussed in the *Handling Missing Data* section.
The proportion of missing values is reported in the `"missing"` column.
If any variables had missing values, we would report an MCAR test with `mice::mcar()`,
and explain that missing data are accounted for using FIML.
In our example, we see that there are no missing values, hence we
proceed with our analysis.

In this case, no preprocessing is necessary:
all variables are on the same 3-point scale.
For an example on LCA with extremely skewed variables, see the LCGA vignette.

## Conducting Latent Class Analysis

Before we fit a series of LCA models, we set a random seed using
`set.seed()`.
This is important because there is some inherent randomness in the estimation procedure,
and using a seed ensures that we (and others) can exactly reproduce the results.

Next, we fit the LCA models.
As all variables are ordinal, we can use the convenience function
`tidySEM::mx_lca()`, which is a wrapper for the generic function `mx_mixture()` optimized for latent class analysis with ordinal data.
Any mixture model can be specified through `mx_mixture()`.
At the time of writing, there are two other wrapper functions for special cases:
`mx_profiles()`, for latent profile analysis, and `mx_growth_mixture()`, for latent growth analysis and growth mixture models.
All of these functions have arguments `data` and number of `classes`.
All variables in `data` are included in the analysis,
so relevant variables must be selected first.
Below, we fit 1 to 4 class solutions:

```{r fitting_LCA, include = TRUE, eval=F}
set.seed(123) # setting seed 
res <- mx_lca(data = df, classes = 1:4) # fitting LCA 1 to 4 class solutions
```

In this example, all models converge without issues.
If this is not the case, however, we can try to aid convergence using `mxTryHard()`, which expands the search for optimal parameter values,
and related functions, `mxTryHardWideSearch()`, which considers a wider range of potential parameter values,
and `mxTryHardOrdinal()` for models with ordinal indicators.
We can inspect the class of resulting objects.

```{r, include = TRUE, eval=F, echo = F}
is(res)
is(res[[1]])
```


<!-- Depending on your computer's computational power, -->
<!-- this might take a while. -->
<!-- As `tidySEM::mx_lca()` run several models at a time, the resulting object `res`  -->
<!-- is a `mixture_list` class where each $K$ class solution is a list element, or -->
<!-- more specifically an `OpenMx` model (`MxModel`). -->


## Class Enumeration

In class enumeration, we want to compare a sequence of LCA models fitted
to the data. To aid the process, we create a model fit table using
`tidySEM::table_fit()` with the results object as the input. As the
output contains a lot of information on each of the four fitted models,
we select a subset of helpful model fit indices and classification
diagnostics.

```{r fit_table, include = TRUE, eval=F}
fit_table <- table_fit(res) # model fit table
fit_table[ , c("Name", "LL", "Parameters", 
               "AIC", "BIC", "Entropy", 
               "prob_min", "prob_max", 
               "n_min", "n_max")] # our selection
```

Our selection of fit indices and classification diagnostics includes:

```{r, echo=F}
Selection <- c(
    "Name", "LL", "Parameters", 
    "AIC", "BIC", "prob_min", 
    "prob_max", "n_min", "n_max")

Description <- c(
    "Number of classes $K$",
    "Basic fit measure of the natural log of the likelihood of the data",
    "Total number of model parameters (in all classes)",
    "Relative fit measure: Akaike Information Criterion",
    "Relative fit measure: Bayesian Information Criterion",
    "Lowest average posterior class probability for class cases were assigned to",
    "Highest average posterior class probability for class cases were assigned to",
    "Proportion of cases in the smallest class based on posterior class probability",
    "Proportion of cases in the largest class based on posterior class probability")

selection <- cbind(Selection, Description)

knitr::kable(selection, format = 'pipe', 
             caption = "Selection of Fit Indices and 
                        Classification Diagnostics")
```


We discussed several possible strategies to select the final class
solution. Here, we apply our own.

To aid our interpretation of the results, we create an elbow plot
showing the trends in information criteria across four models.

```{r elbow_plot, include = TRUE, message=F, warning=F, eval=F}
library(tidyverse) # for data-wrangling
library(ggplot2) # for plots

elbow_plot <- fit_table[ , c("Name", "AIC", "BIC")] # extract ICs
elbow_plot <- pivot_longer(elbow_plot, cols = c("AIC", "BIC"), 
                           names_to = "IC", values_to = "Value") # to long format

ggplot(elbow_plot, aes(x = Name, y = Value, group = IC))+
  geom_point(aes(color = IC))+
  geom_line(aes(color = IC))+
  labs(title="Elbow Plot of Information Criteria per Class Solution", 
       x = "Class Solution", y = " Value")+
  scale_color_manual(name = "Information Criterion", 
                     values = c(AIC = 'blue', BIC = 'red'))+
  theme_minimal()

```

From the elbow plot, we see that AIC has a lower penalty for model
complexity. However, we are more interested in the BIC values, which are
similar for the one, two and three-class solutions, but the four-class
solution fits significantly worse. For this reason, we eliminate the
four-class solution from the selection process.

Then we examine the model fit table. As expected, the -2\*log likelihood
falls successively with each added class. As previously stated,
classification diagnostics should not be used for model selection, but
they can be used to disqualify certain solutions because they are
uninterpretable. We see that prob_min and n_min for the four-class solution is
low, knowing that this solution also has a high BIC, we disqualify this
solution.

Out of the remaining three solutions, we notice that entropy is the
highest for the three-class solution, and it has a satisfactory prob_min
and n_min. Based on this, we retain the three class solution in model
selection. Note that entropy for the one-class solution will always
equal to one, as it is 100% true that every case is in that class. Based
on the low entropy of the two-class solution, we eliminate this model.

Finally, when comparing the one and three-class solutions, we inspect
the information criteria. For BIC, the one-class solution fits better,
but the difference is marginal. AIC tells us that the added complexity
of having three classes still explains the data better than a one-class
solution. Therefore, we select the three-class solution as our
final-class solution.

## Interpreting the Final Class Solution

To aid our understanding of the final class solution, we use
`ggplot2::plot_prob()` with the results of the three-class model as the
input. The resulting graph shows response patterns on all the indicators
for each group.

If we want to know the probability of each response option's endorsement
for each class, we can use `tidySEM::table_prob`. These are thresholds
for ordinal dependent variables in the probability scale.

```{r plot_prob, include = TRUE, eval=F}
plot_prob(res[[3]]) # visualizing the response patterns for the final model
table_prob(res[[3]]) # tabulating the response patterns for the final model
```

In the plot, we can see the distributions of the response probabilities
on the indicators for each of the three classes. For instance, we see
that in Class 1 the most common response to u2 is 2, while in Class 2
and Class 3 this is 0. We can also see that response 1 is a rare
response not forming the majority in any class. Class 2 distinguishes
itself because the majority scores the response 0 category of u3 and u4,
while in Class 1 and 2 this is not the case. Class 3 distinguishes
itself because the most common response to u3 and u4 is 3.

We can also interpret the response patterns numerically. It is a matter of 
preferences on how to interpret these probabilities. Here is where you would
**name** each class, such that each response pattern is theoretically meaningful.

## Extracting Posterior Class Probabilities

Another step is to extract posterior class probabilities. This is done
by the use of `tidySEM::class_prob` with the results of the final class
solution as the input.

```{r extract_Post_Class_Prob, include = TRUE, eval=F}
probs <- class_prob(res[[3]]) # extracting the posterior class probabilities
probs$mostlikely.class # posterior probabilities by most likely class membership
probs$individual # individual posterior class probabilities
```

\newpage

# References
